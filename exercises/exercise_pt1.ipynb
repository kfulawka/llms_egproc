{"cells":[{"cell_type":"markdown","id":"979416b3c30fb86b","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"979416b3c30fb86b"},"source":["## Using Notebook Environments\n","1. To run a cell, press `shift + enter`. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell (text only), it will render the markdown and move to the next cell.\n","2. Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the kernel by clicking Runtime in the menu bar (if you're using Colab) and selecting `Restart runtime`. This will clear all variables and outputs.\n","3. The final variable in a cell will be printed on the screen. If you want to print multiple variables, use the `print()` function as usual.\n","\n","Notebook environments support code cells and markdown (text) cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with `#`)"]},{"cell_type":"markdown","id":"c616368742ccde73","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"c616368742ccde73"},"source":["## Environment Setup\n","The code below allows the notebook to acces your google files, including reading the files in, and writing files in specified locations.\n","Once you run it, you'll have to give the notebbok the access to a google drive of your choosen google account."]},{"cell_type":"code","source":["# mount googl drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"id":"tcx3KlNmeU3H"},"id":"tcx3KlNmeU3H","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"290c24c2ed829f80","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"290c24c2ed829f80"},"source":["We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the `import <name> as <nickname>` syntax (e.g. `import pandas as pd`). These are usually standardized nicknames.\n","\n","We'll use following packages:\n","\n","1. `pandas`: A very popular package for reading and manipulating data in python.\n","2. `huggingface_hub`: A package for extracting features from text data using\n","3. `re` for string matching\n","4. `textwrap` and `IPython` to make the display of some of the tables more readable\n","5. `pandas` is a popular package for data frames manipulation\n","6. `huggingface_hub` for making API calls and prompt the LLMs"]},{"cell_type":"code","execution_count":4,"id":"initial_id","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"id":"initial_id","executionInfo":{"status":"ok","timestamp":1722246520566,"user_tz":-120,"elapsed":5096,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}}},"outputs":[],"source":["import sys\n","import os\n","import re\n","import textwrap\n","from IPython.display import display, HTML\n","import pandas as pd\n","from huggingface_hub import InferenceClient\n","\n","# the code below installs huggingface hub if it's missing\n","if 'google.colab' in sys.modules:  # If in Google Colab environment\n","\n","    # Installing requisite packages\n","    !pip install huggingface_hub &> /dev/null\n","\n","# this sets the working directory to the exercises folder\n","base_path = '/content/drive/My Drive/llms_egproc/exercises/'\n","os.chdir(base_path)"]},{"cell_type":"markdown","id":"b8df0bbd","metadata":{"id":"b8df0bbd"},"source":["# Identifying decision reasons (part 1)\n","In this exercise, we will explore the capabilities of LLMs to identify decision reasons in verbal reports using the Hugging Face (HF) ecosystem.\n","\n","By the end of this exercise, you will have learned how to:\n","- Design a zero-shot prompt\n","- Have large models on the Hugging Face servers evaluate your prompts  \n","- Validate the model output"]},{"cell_type":"markdown","id":"f6bf097acf12ea11","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"f6bf097acf12ea11"},"source":["## Getting access to a Large Language Model\n","Most best LLMs are to large to just simply be downloaded and run on your local machine. One soultion to get access to LLMs like GPT-4 or LLAMA-3-70b, is to call LLMs directly via an API. We will use access provided by Hugging Face (HF) company (note: you need a Pro account to get access to lare models like LLAMA-3-70b).\n","\n","We start by settiing up the inference client. The `InferenceClient` function from huggingface ecosystem gives easy access to hundres of LLMs.\n","The main two arguments of the function are:\n","* `model`: the model name (see https://huggingface.co/meta-llama for LLAMA models avilable through HF)\n","* `token`: This is your personal token which authenticates your access to the serivce.\n"]},{"cell_type":"code","source":["# paste your token here\n","API_TOKEN = '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'\n","# we'll use the LLAMA-3 model, version with 70 Bilion parameters\n","LLAMA_version = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n","# pass model version and the token to the InferenceClient function and save the output under some name, e.g., LLAMA\n","LLAMA = InferenceClient(model = LLAMA_version, token = API_TOKEN)"],"metadata":{"id":"HlFQGLUDi_gD","executionInfo":{"status":"ok","timestamp":1722259147407,"user_tz":-120,"elapsed":435,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}}},"id":"HlFQGLUDi_gD","execution_count":38,"outputs":[]},{"cell_type":"markdown","source":["## Using the InferenceClient\n","Now we can use the `LLAMA` object to prompt the model. For our purposes, we will focus on the `.text_generation` method, like in the code block below.\n","1. run the code and investigate the output\n","2. change the max_new_tokes argument to lower value, e.g., 100 and run again\n","3. change the content of the stupdid prompt"],"metadata":{"id":"zc33BH8bwWR-"},"id":"zc33BH8bwWR-"},{"cell_type":"code","source":["# let's create some stupid prompt and save it under informative name\n","stupid_prompt = 'Give an example of a stupid prompt'\n","\n","# Get a response from the Meta-Llama-3-70B-Instruct saved under LLAMA object\n","# The max_new_tokens provided the limit on the output lenght\n","stupid_response = LLAMA.text_generation(prompt = stupid_prompt, max_new_tokens = 4000)\n","print(stupid_response)"],"metadata":{"id":"Jgdi8rXOjUHl"},"id":"Jgdi8rXOjUHl","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### System and User messages\n","Lllama uses special tokens to distinguish between system and user parts of the prompt.\n","Run the code below and have a look at the full prompt.\n"],"metadata":{"id":"5R0AEL-Jwn__"},"id":"5R0AEL-Jwn__"},{"cell_type":"code","source":["# system message sets the role to a decision scientis\n","system_role = \"\"\"\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","You are an expert decision scientist\n","<|eot_id|>\"\"\"\n","\n","user_question = \"\"\"\n","<|start_header_id|>user<|end_header_id|>\n","What is the best way to make financial decisions?\n","<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n","\n","# combine both into a single prompt\n","full_prompt = system_role + user_question\n","\n","# print the full prompt\n","print(full_prompt)"],"metadata":{"id":"_l7f3Avpkiv_"},"id":"_l7f3Avpkiv_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prompting the model\n","Try the prompt with the model role set to decision scientist.\n","\n","1.   Try the prompt with the model role set to decision scientist.\n","2.   Now try different roles:\n","        *   Priest\n","        *   Economist\n","        *   Generic grandpa"],"metadata":{"id":"sld79yiZw8cy"},"id":"sld79yiZw8cy"},{"cell_type":"code","source":["response = LLAMA.text_generation(full_prompt, max_new_tokens = 200)\n","print(response)"],"metadata":{"id":"1SMSSFtMmP48"},"id":"1SMSSFtMmP48","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"efc8d5804ede6511","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"efc8d5804ede6511"},"source":["## Identyfing decision reasons --- prompt template\n","Now have a look at our prompt. Read in the `prompt_template` from the google drive.\n","1. Make sure you understand all parts.\n","2. Which prompting techinques can you spot?"]},{"cell_type":"code","execution_count":null,"id":"fbb44bcf-bb8a-4603-8c25-33b1537399e6","metadata":{"collapsed":true,"id":"fbb44bcf-bb8a-4603-8c25-33b1537399e6"},"outputs":[],"source":["# Read the File\n","prompt_path = 'prompts/prompt_v1.txt'\n","\n","# Open the file and read its contents\n","with open(prompt_path, 'r') as file:\n","    prompt_template = file.read()\n","\n","print(prompt_template)"]},{"cell_type":"markdown","id":"4830e24a5d45430d","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"4830e24a5d45430d"},"source":["## Constructing the prompt\n","The single complete prompt for identifying the deicion reason has to include:\n","1. Definitions of key terms\n","2. Instructions on how to perform the identification\n","3. The DECISION REASON\n","4. The DESICION PROBLEM\n","5. The VERBAL REPORT\n","\n","The code below reads in the problems, reasons, and reports from your google drive.\n"]},{"cell_type":"code","source":["# read in decision problems, decision reasons, and verbal reports\n","decision_problems = pd.read_csv('data/decision_problems.csv', encoding = 'utf-8')\n","decision_reasons = pd.read_csv('data/decision_reasons.csv', encoding = 'utf-8')\n","verbal_reports = pd.read_csv('data/verbal_reports.csv', encoding = 'utf-8')\n","\n","# merge verbal reports with decision problems\n","problems_reports = pd.merge(decision_problems, verbal_reports, on = 'problem_id')\n","# print(problems_reports)\n","# print(decision_reasons)"],"metadata":{"id":"LTbks_7t0_fI","executionInfo":{"status":"ok","timestamp":1722248386848,"user_tz":-120,"elapsed":1150,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}}},"id":"LTbks_7t0_fI","execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["### Function for creating the full prompt\n","The function below tahes as inputs the `prompt_template` and fills the placeholders with corresponding elemnts.\n","Run the code cell to read in the function into the environment."],"metadata":{"id":"DbzHa9ze_I09"},"id":"DbzHa9ze_I09"},{"cell_type":"code","source":["# function for constructing the full prompt\n","def generate_prompt(prompt_template, decision_problem, decision_reason, verbal_report):\n","    \"\"\"\n","    Replaces placeholders in the prompt with the given decision problem, decision reason, and verbal report.\n","    \"\"\"\n","    # Replace placeholders with actual values\n","    full_prompt = prompt_template.replace(\"DECISION_PROBLEM\", decision_problem)\n","    full_prompt = full_prompt.replace(\"DECISION_REASON\", decision_reason)\n","    full_prompt = full_prompt.replace(\"VERBAL_REPORT\", verbal_report)\n","\n","    return full_prompt"],"metadata":{"id":"yA7j1zok7pl_","executionInfo":{"status":"ok","timestamp":1722248417056,"user_tz":-120,"elapsed":307,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}}},"id":"yA7j1zok7pl_","execution_count":20,"outputs":[]},{"cell_type":"markdown","source":["### Create the prompts\n","Now we will create a set of prompts for the maximum outcome decision reason."],"metadata":{"id":"Sdhzqhl5_rV2"},"id":"Sdhzqhl5_rV2"},{"cell_type":"code","source":["# Select the description for expected value\n","selected_reason = 'maximum outcome'\n","\n","# get the description of the reason from the decision_reasons data frame\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]\n","\n","# Create a list for storing prompts for the expected value reason\n","maximum_outcome_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","# this loops over each row of data with verbal reports and corrsponding decision problems\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_template,\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    maximum_outcome_prompts.append(prompt)"],"metadata":{"id":"C-_hO7wnHgsj","executionInfo":{"status":"ok","timestamp":1722259786307,"user_tz":-120,"elapsed":315,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}}},"id":"C-_hO7wnHgsj","execution_count":45,"outputs":[]},{"cell_type":"code","source":["# have a look at the first full prompt\n","# you can investigate other prompts by changing the number from 0 to some other value\n","print(maximum_outcome_prompts[0])"],"metadata":{"id":"MnKgi_FiKt4r"},"id":"MnKgi_FiKt4r","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"81690eb2b6590715","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"81690eb2b6590715"},"source":["## Identifing decision reasons: a test\n","\n","- run first prompt\n","- make sense of output relative to prompt\n","- maybe try again or next prompt\n"]},{"cell_type":"code","source":["# pass the first prompt to LLAMA and save the output\n","result = LLAMA.text_generation(maximum_outcome_prompts[0], max_new_tokens = 4000)"],"metadata":{"id":"bDvebdMDM9I1","executionInfo":{"status":"ok","timestamp":1722261600727,"user_tz":-120,"elapsed":385,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}}},"id":"bDvebdMDM9I1","execution_count":78,"outputs":[]},{"cell_type":"code","source":["# print the llama evaluation\n","print(result[0])"],"metadata":{"id":"wUZDnpOdNTKd"},"id":"wUZDnpOdNTKd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extractig confidence ratings\n","\n","### Function for extracting the confidence rating\n","As you can see in the output above, following our prompt the model provides full descrrpition of the deliberation process and the confidence assesment at the end."],"metadata":{"id":"bLQ95iRBP798"},"id":"bLQ95iRBP798"},{"cell_type":"code","source":["# Function for extracting confidence assessments\n","def extract_confidence(s):\n","    \"\"\"\n","    Extracts an integer value from a string enclosed between @ or @@ symbols.\n","    \"\"\"\n","    # Regular expression to match patterns like @number@ or @@number@@\n","    pattern = r'@+(\\s*\\d+\\s*)@+'\n","\n","    # Search for the pattern in the string\n","    match = re.search(pattern, s)\n","\n","    if match:\n","        # Extract the number and convert it to an integer\n","        number_str = match.group(1).strip()\n","        return int(number_str)\n","\n","    return None"],"metadata":{"id":"qD2itiT6RkvJ","executionInfo":{"status":"ok","timestamp":1722252220266,"user_tz":-120,"elapsed":234,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}}},"id":"qD2itiT6RkvJ","execution_count":27,"outputs":[]},{"cell_type":"code","source":["# test the extract_confidence function\n","print(extract_confidence('something else @10 @ xxx'))\n","print(extract_confidence('something else @@13@@ xxx'))\n","print(extract_confidence(result))"],"metadata":{"id":"FelpKr90WFhK"},"id":"FelpKr90WFhK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Example Analysis\n","Now you'll run the analysis on the etire data set. For each verbal report and decision problem combination, the model with provide assessment of confidence on whether the individual used the expected value reason.\n","\n","### Maximum Outcome\n","We will iterate over the entier list with prompts containinig the **maximum outcome** reason (stored in the `maximum_outcome_prompts`). On each iteration the model will assess if **maximum outcome*** reason was used by the individual, based on the verbal report.\n","\n","Specifically:\n","1. We pass a `prompt` to the model\n","2. The full output from the LLAMA , `llama_response` is saved in a list `maximum_outcome_eval` for later inspection\n","3. We use the `extract_confidence` function to extract the confidence assesment from `llama_response`\n","4. We save the `confidence_assesment` in the new colmun of the data set with decision problems abnd verbal reports `problems_reports['maximum outcome']`\n"],"metadata":{"id":"jDtlTVQ0WDe3"},"id":"jDtlTVQ0WDe3"},{"cell_type":"code","source":["# list for storing the output from the LLAMA model\n","maximum_outcome_eval = []\n","\n","# analyzed reason\n","analyzed_reason = \"maximum outcome\"\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","# remind that selected reason was set to 'expected value'\n","problems_reports[analyzed_reason] = None\n","\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(maximum_outcome_prompts):\n","\n","    # response from LLAMA\n","    llama_response = LLAMA.text_generation(prompt, max_new_tokens = 4000)\n","    maximum_outcome_eval.append(llama_response) # save the response to the expected_value_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(llama_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, analyzed_reason] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]))"],"metadata":{"id":"ufnZmd5pUR1w"},"id":"ufnZmd5pUR1w","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Displayin the results\n","The functions below are not too imporant. Their only goal is to display the tables in the notebbok in a nice, HTML format, which is easier to read than the base output of `print()`."],"metadata":{"id":"lu1B6Rh3CORX"},"id":"lu1B6Rh3CORX"},{"cell_type":"code","source":["# Function to wrap text\n","def wrap_text(text, width=100):\n","    return \"<br>\".join(textwrap.wrap(text, width))\n","\n","# display data frames in HTML\n","def disp_tab(dd):\n","    dd = dd.to_html(escape=False)\n","    return display(HTML(dd))\n","\n","# Function to show verbal reports with assigned numbers in a specified range\n","def show_verbal_reports_in_range(data, reason, min_confidence, max_confidence):\n","    \"\"\"\n","    Shows verbal reports for which the model assigned a confidence within the specified range.\n","    \"\"\"\n","    filtered_data = data[(data[reason] >= min_confidence) & (data[reason] <= max_confidence)] # filter by the specified range\n","\n","     # wrap the text for nicer display\n","    filtered_data.loc[:, 'verbal_report'] = filtered_data['verbal_report'].apply(wrap_text)\n","    filtered_data.loc[:, 'decision_problem'] = filtered_data['decision_problem'].apply(lambda x: wrap_text(x, width=40))\n","\n","    # select only the columns with report and confidence assesment\n","    filtered_data = filtered_data[['decision_problem', 'verbal_report', 'choice', reason]]\n","    filtered_data = filtered_data.to_html(escape=False) # to html\n","\n","    return display(HTML(filtered_data))\n","    # return filtered_data[['verbal_report', reason]]"],"metadata":{"id":"YE4WDuE3Z5Iv","executionInfo":{"status":"ok","timestamp":1722262068270,"user_tz":-120,"elapsed":264,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}}},"id":"YE4WDuE3Z5Iv","execution_count":85,"outputs":[]},{"cell_type":"markdown","source":["#### High confidence assesments\n","The code below displays the verbal reports for which the LLM thought that there is a HIGH chance that the **maximum outcome** reason was used when making the decision."],"metadata":{"id":"POycQ0qnqwQ5"},"id":"POycQ0qnqwQ5"},{"cell_type":"code","source":["# Show verbal reports for which the Expected value reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'maximum outcome', 80, 100)"],"metadata":{"id":"-2rC48bLeeph"},"id":"-2rC48bLeeph","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Low confidence assesments\n","The code below displays the verbal reports for which the LLM thought that there is a LOW chance that the **maximum outcome** reason was used when making the decision."],"metadata":{"id":"bB7FdZuFq_sG"},"id":"bB7FdZuFq_sG"},{"cell_type":"code","source":["# Show verbal reports for which the Expected value reason was assessed not to be used---i.e., confidence in usuing th reason was low, between 0 and 20\n","show_verbal_reports_in_range(problems_reports, 'maximum outcome', 0, 20)"],"metadata":{"id":"Y2Vz_ETwewnT"},"id":"Y2Vz_ETwewnT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Full data\n","You can view the entire data set by running the code below."],"metadata":{"id":"IvDLg1NIrGBc"},"id":"IvDLg1NIrGBc"},{"cell_type":"code","source":["# Show all results\n","show_verbal_reports_in_range(problems_reports, analyzed_reason, 0, 100)"],"metadata":{"id":"Vh2srIRoljtj"},"id":"Vh2srIRoljtj","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### LLM reasoninig\n","Full LLM output was saved in the `maximum_outcome_eval` object. You can acces it by printing the elements one at a time. Notice that the output tables above provide row numbers in the leftmost column. These numbers corresponds to the entries in the `maximum_outcome_eval` object.\n","\n","Thus, if you want to display the LLM deliberatino process from which the assesment in row 1 was taken, you simply run:"],"metadata":{"id":"wMkKU-A-rMyF"},"id":"wMkKU-A-rMyF"},{"cell_type":"code","source":["print(maximum_outcome_eval[1])"],"metadata":{"id":"cS9CA-SVraCT"},"id":"cS9CA-SVraCT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# SURE OUTCOME\n","Run the analyses for the **sure outcome** decision reason.\n","\n","## Set up the reason name and description"],"metadata":{"id":"sdlpW7AlmLLQ"},"id":"sdlpW7AlmLLQ"},{"cell_type":"code","source":["# Select the description for expected value\n","analyzed_reason = \"sure outcome\"\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == analyzed_reason, 'decision reason description'].values[0]\n","print(selected_description)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WsK2JWigs7v5","executionInfo":{"status":"ok","timestamp":1722260801869,"user_tz":-120,"elapsed":300,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}},"outputId":"8fc795a7-3f77-4a63-d01c-d38040ab9742"},"id":"WsK2JWigs7v5","execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["The reason considers the presence of a sure outcome, that is an outcome with 100% probability, of each lottery. The reason prefers the lottery with or without the sure outcome, depending on whether the sure outcome is a favorable outcome in the context of all possible outcomes.\n"]}]},{"cell_type":"markdown","source":["## Sure Outcome Prompts\n","Create the list with prompts contatinig **sure outcome** reason"],"metadata":{"id":"YTOuqci6nAbw"},"id":"YTOuqci6nAbw"},{"cell_type":"code","source":["# Create a list for storing prompts for the expected value reason\n","sure_outcome_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_template,\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    sure_outcome_prompts.append(prompt)\n","\n","print(sure_outcome_prompts[0])"],"metadata":{"id":"mVnjA_V8mZrS"},"id":"mVnjA_V8mZrS","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sure Outcome LLAMA evaluation"],"metadata":{"id":"sy28K58XnD3M"},"id":"sy28K58XnD3M"},{"cell_type":"markdown","source":["#### Run the LLM on a random prompt"],"metadata":{"id":"A00i1bzuvwel"},"id":"A00i1bzuvwel"},{"cell_type":"code","source":["sure_outcome_res = LLAMA.text_generation(prompt = sure_outcome_prompts[5], max_new_tokens=4000),"],"metadata":{"id":"AfPOIIyev4AZ","executionInfo":{"status":"ok","timestamp":1722260991419,"user_tz":-120,"elapsed":10500,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}}},"id":"AfPOIIyev4AZ","execution_count":62,"outputs":[]},{"cell_type":"code","source":["print(sure_outcome_res[0])"],"metadata":{"id":"n1FfRpG_wXY_"},"id":"n1FfRpG_wXY_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Run the LLM on the entire list of prompts"],"metadata":{"id":"kC6KPO2pyt8d"},"id":"kC6KPO2pyt8d"},{"cell_type":"code","source":["# list for storing the output from the LLAMA model\n","sure_outcome_eval = []\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","# remind that selected reason was set to 'expected value'\n","problems_reports[analyzed_reason] = None\n","\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(sure_outcome_prompts):\n","\n","    # response from LLAMA\n","    llama_response = LLAMA.text_generation(prompt, max_new_tokens = 4000)\n","    sure_outcome_eval.append(llama_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(llama_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, analyzed_reason] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"id":"FLk9D_u-nIZB"},"id":"FLk9D_u-nIZB","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### High confidence assesments for sure outcome reason"],"metadata":{"id":"UQjoJc3Ty37Z"},"id":"UQjoJc3Ty37Z"},{"cell_type":"code","source":["# Show verbal reports for which the Sure outcome reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'sure outcome', 80, 100)"],"metadata":{"id":"4mDf7-ZZn7Bs"},"id":"4mDf7-ZZn7Bs","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Low confidence assesments for sure outcome reason"],"metadata":{"id":"0AokAxEMzGnc"},"id":"0AokAxEMzGnc"},{"cell_type":"code","source":["# Show verbal reports for which the Sure outcome reason was assessed not to be used---i.e., confidence in usuing th reason was low, between 0 and 20\n","show_verbal_reports_in_range(problems_reports, 'sure outcome', 0, 20)"],"metadata":{"id":"fGK3fCVSnuPD"},"id":"fGK3fCVSnuPD","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### The full LLM reasoninig for selected data point\n","Change the number to view reasoninig associated with the data point of interest"],"metadata":{"id":"ycVQ6FhvzTnZ"},"id":"ycVQ6FhvzTnZ"},{"cell_type":"code","source":["print(maximum_outcome_eval[1])"],"metadata":{"id":"SVpHFAVUzb6K"},"id":"SVpHFAVUzb6K","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recrating the analysis on your own\n","If you have the time, have a look at the list of reasons we prepared for you. Selecet a reason that you like (or don't like) and try to recrated the analysis for it.\n","\n","Good luck!"],"metadata":{"id":"GMNFQCP-zn6l"},"id":"GMNFQCP-zn6l"},{"cell_type":"code","source":["disp_tab(decision_reasons)"],"metadata":{"id":"k5WSvjW5z88U"},"id":"k5WSvjW5z88U","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}