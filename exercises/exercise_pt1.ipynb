{"cells":[{"cell_type":"markdown","id":"979416b3c30fb86b","metadata":{"collapsed":false,"id":"979416b3c30fb86b","jupyter":{"outputs_hidden":false}},"source":["## Using Notebook Environments\n","1. To run a cell, press `shift + enter`. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell (text only), it will render the markdown and move to the next cell.\n","2. Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the kernel by clicking Runtime in the menu bar (if you're using Colab) and selecting `Restart runtime`. This will clear all variables and outputs.\n","3. The final variable in a cell will be printed on the screen. If you want to print multiple variables, use the `print()` function as usual.\n","\n","Notebook environments support code cells and markdown (text) cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with `#`)"]},{"cell_type":"markdown","id":"c616368742ccde73","metadata":{"collapsed":false,"id":"c616368742ccde73","jupyter":{"outputs_hidden":false}},"source":["## Environment Setup\n","The code below allows the notebook to acces your google files, including reading the files in, and writing files in specified locations.\n","Once you run it, you'll have to give the notebbok the access to a google drive of your choosen google account."]},{"cell_type":"code","execution_count":null,"id":"tcx3KlNmeU3H","metadata":{"id":"tcx3KlNmeU3H"},"outputs":[],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","id":"290c24c2ed829f80","metadata":{"collapsed":false,"id":"290c24c2ed829f80","jupyter":{"outputs_hidden":false}},"source":["We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the `import <name> as <nickname>` syntax (e.g. `import pandas as pd`). These are usually standardized nicknames.\n","\n","We'll use following packages:\n","\n","1. `sys` and `os` for managing package installations and directories\n","2. `re` for string matching\n","3. `textwrap` and `IPython` to make the display of some of the tables more readable\n","4. `pandas` is a popular package for data frames manipulation\n","5. `huggingface_hub` for making API calls and prompt the LLMs"]},{"cell_type":"code","execution_count":null,"id":"initial_id","metadata":{"collapsed":true,"id":"initial_id","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["import sys\n","import os\n","import re\n","import textwrap\n","from IPython.display import display, HTML\n","import pandas as pd\n","from huggingface_hub import InferenceClient\n","\n","# the code below installs huggingface hub if it's missing\n","if 'google.colab' in sys.modules:  # If in Google Colab environment\n","\n","    # Installing requisite packages\n","    !pip install huggingface_hub &> /dev/null\n","\n","# this sets the working directory to the exercises folder\n","os.chdir('/content/drive/My Drive/llms_egproc/exercises/')"]},{"cell_type":"markdown","id":"b8df0bbd","metadata":{"id":"b8df0bbd"},"source":["# Identifying decision reasons\n","In this exercise, we will explore the capabilities of LLMs to identify decision reasons in verbal reports using the Hugging Face (HF) ecosystem.\n","\n","By the end of this exercise, you will have learned how to:\n","- Design a zero-shot prompt\n","- Have large models on the Hugging Face servers evaluate your prompts  \n","- Validate the model output"]},{"cell_type":"markdown","id":"f6bf097acf12ea11","metadata":{"collapsed":false,"id":"f6bf097acf12ea11","jupyter":{"outputs_hidden":false}},"source":["## Getting access to a Large Language Model\n","Powerful LLMs like LLAMA-3-70b and even its smaller siblings are typically too large to download and run on your local machine. One alternative solution to get access to LLMs is to use APIs that run the LLMs on remote servers. In this exercise, we will use the API provided by Hugging Face, which is the central actor in the open language model sphere.\n","\n","In addition to offering hundreds of thousands of LLMs, including the most powerful open models developed by Meta, Google, and others, Hugging Face offers a collection of Python libraries to facilitate the use of open LLMs. We will make use of the `huggingface_hub` library to interact with the models running on Hugging Face's servers.  \n","\n","We start by setting up the API `InferenceClient` which we have loaded from the `huggingface_hub` library in the previous chunk. The function takes two argumentsare:\n","* `model`: the model name (see, e.g., https://huggingface.co/meta-llama for collection of Llama models)\n","* `token`: your personal access token obtained through your Hugging Face account.\n"]},{"cell_type":"code","execution_count":null,"id":"HlFQGLUDi_gD","metadata":{"id":"HlFQGLUDi_gD"},"outputs":[],"source":["# paste your token here\n","API_TOKEN = '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'\n","\n","# we'll use latest, small LLAMA-3 model\n","LLAMA_version = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","\n","# pass model version and the token to the InferenceClient function and save the output under some name, e.g., LLAMA\n","LLAMA = InferenceClient(model = LLAMA_version, token = API_TOKEN)"]},{"cell_type":"markdown","id":"zc33BH8bwWR-","metadata":{"id":"zc33BH8bwWR-"},"source":["## Using the InferenceClient\n","Now, you can use `.text_generation` method of the `LLAMA` object to prompt the model. It takes the prompt and a `max_new_tokens` argument controlling the size of the text output as inputs.\n","1. run the chunk and investigate the output.\n","2. try other prompts."]},{"cell_type":"code","execution_count":null,"id":"Jgdi8rXOjUHl","metadata":{"id":"Jgdi8rXOjUHl"},"outputs":[],"source":["# let's create some prompt\n","prompt = 'Once upon a time'\n","\n","# Get a response from the Meta-Llama-3.1-8B-Instruct\n","response = LLAMA.text_generation(prompt = prompt, max_new_tokens = 300)\n","print(response)"]},{"cell_type":"markdown","id":"5R0AEL-Jwn__","metadata":{"id":"5R0AEL-Jwn__"},"source":["Being an assistant model, Llama distinguishes between system and user messages of the prompt. The two are separated using a collection of special tokens. The code below shows an example with special tokens. Run the prompt as is and study the result. Then change the system role to one of the following and run again:\n","*   Priest\n","*   Economist\n","*   Generic grandpa"]},{"cell_type":"code","execution_count":null,"id":"_l7f3Avpkiv_","metadata":{"id":"_l7f3Avpkiv_"},"outputs":[],"source":["# system message sets the role to a decision scientis\n","prompt = \"\"\"\n","<|begin_of_text|>\n","<|start_header_id|>system<|end_header_id|>\n","You are an expert decision scientist\n","<|eot_id|>\n","<|start_header_id|>user<|end_header_id|>\n","What is the best way to make financial decisions?\n","<|eot_id|>\n","<|start_header_id|>assistant<|end_header_id|>\"\"\"\n","\n","# run prompt\n","response = LLAMA.text_generation(prompt, max_new_tokens = 200)\n","print(response)"]},{"cell_type":"markdown","id":"efc8d5804ede6511","metadata":{"collapsed":false,"id":"efc8d5804ede6511","jupyter":{"outputs_hidden":false}},"source":["## Piecing the prompt together\n","In this section, we will build a prompt for reason identification. Let's first take a look at our `prompt_template`. Read it from your google drive and study its individual components. Which prompting techniques do you see at work?"]},{"cell_type":"code","execution_count":null,"id":"fbb44bcf-bb8a-4603-8c25-33b1537399e6","metadata":{"collapsed":true,"id":"fbb44bcf-bb8a-4603-8c25-33b1537399e6","jupyter":{"outputs_hidden":true}},"outputs":[],"source":["# Read the File\n","prompt_path = 'prompts/prompt_v1.txt'\n","\n","# Open the file and read its contents\n","with open(prompt_path, 'r') as file:\n","    prompt_template = file.read()\n","\n","print(prompt_template)"]},{"cell_type":"markdown","id":"4830e24a5d45430d","metadata":{"collapsed":false,"id":"4830e24a5d45430d","jupyter":{"outputs_hidden":false}},"source":["To complete the prompt, the following three pieces of information need to be replaced for each trial. This information will come from files on your drive.\n","\n","1. The DECISION REASON\n","2. The DECISION PROBLEM\n","3. The VERBAL REPORT\n","\n","The code below reads the files with this information and specifies a function that will help us fill in this information efficiently."]},{"cell_type":"code","execution_count":null,"id":"LTbks_7t0_fI","metadata":{"id":"LTbks_7t0_fI"},"outputs":[],"source":["# read in decision problems, decision reasons, and verbal reports\n","decision_problems = pd.read_csv('data/decision_problems.csv', encoding = 'utf-8')\n","decision_reasons = pd.read_csv('data/decision_reasons.csv', encoding = 'utf-8')\n","verbal_reports = pd.read_csv('data/verbal_reports.csv', encoding = 'utf-8')\n","\n","# merge verbal reports with decision problems\n","problems_reports = pd.merge(decision_problems, verbal_reports, on = 'problem_id')\n","\n","# function for constructing the full prompt\n","def generate_prompt(prompt_template, decision_problem, decision_reason, verbal_report):\n","\n","    # Replace placeholders with actual values\n","    full_prompt = prompt_template.replace(\"DECISION_PROBLEM\", decision_problem)\n","    full_prompt = full_prompt.replace(\"DECISION_REASON\", decision_reason)\n","    full_prompt = full_prompt.replace(\"VERBAL_REPORT\", verbal_report)\n","\n","    return full_prompt"]},{"cell_type":"markdown","id":"Sdhzqhl5_rV2","metadata":{"id":"Sdhzqhl5_rV2"},"source":["Now, let's complete the first prompt. Use the code below to create the first set of prompts for the maximum outcome decision reason. Run code."]},{"cell_type":"code","execution_count":null,"id":"C-_hO7wnHgsj","metadata":{"id":"C-_hO7wnHgsj"},"outputs":[],"source":["# set maximum outcome as value for the selected_reason variable\n","selected_reason = 'maximum outcome'\n","\n","# get the description of the reason from the decision_reasons data frame\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]\n","\n","# Create a list for storing prompts for the maximum outcome reason\n","maximum_outcome_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","# this loops over each row of data with verbal reports and corrsponding decision problems\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the maximum outcome reason\n","    prompt = generate_prompt(\n","        prompt_template,\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    maximum_outcome_prompts.append(prompt)"]},{"cell_type":"markdown","id":"5281cd68-52be-4942-893b-62e534a7f986","metadata":{"id":"5281cd68-52be-4942-893b-62e534a7f986"},"source":["Have a look at the first full prompt. You can change the index to inspect prompts for other problems and reports."]},{"cell_type":"code","execution_count":null,"id":"MnKgi_FiKt4r","metadata":{"id":"MnKgi_FiKt4r"},"outputs":[],"source":["# you can investigate other prompts by changing the number from 0 to some other value\n","print(maximum_outcome_prompts[0])"]},{"cell_type":"markdown","id":"81690eb2b6590715","metadata":{"collapsed":false,"id":"81690eb2b6590715","jupyter":{"outputs_hidden":false}},"source":["## Testing the identification of reasons\n","\n","In this section, we will try out identifying reasons using the LLM. The code below sends the first maximum outcome prompt to the LLM. Run it and make sense of the output relative to the prompt. You can again try other prompts by changing the the index."]},{"cell_type":"code","execution_count":null,"id":"bDvebdMDM9I1","metadata":{"id":"bDvebdMDM9I1"},"outputs":[],"source":["# pass the first prompt to LLAMA and save the output\n","response = LLAMA.text_generation(maximum_outcome_prompts[0], max_new_tokens = 1000)\n","\n","# print the llama evaluation\n","print(response)"]},{"cell_type":"markdown","id":"bLQ95iRBP798","metadata":{"id":"bLQ95iRBP798"},"source":["You may have picked up that our prompt instructs you to return the confidence for the presence of the reason in a particular format. We do this so that it is easy to extract the confidence value from the output text. The code below defines a simple function to extract the confidence and applies it to a few examples, including the response produced for the first full prompt."]},{"cell_type":"code","execution_count":null,"id":"qD2itiT6RkvJ","metadata":{"id":"qD2itiT6RkvJ"},"outputs":[],"source":["# Function for extracting confidence assessments\n","def extract_confidence(s):\n","\n","    # Regular expression to match patterns like @number@ or @@number@@\n","    pattern = r'@+(\\s*\\d+\\s*)@+'\n","\n","    # Search for the pattern in the string\n","    match = re.search(pattern, s)\n","\n","    if match:\n","        # Extract the number and convert it to an integer\n","        number_str = match.group(1).strip()\n","        return int(number_str)\n","\n","    return 999 # if match was not found, the function will return 999\n","\n","# test the extract_confidence function\n","print('test 1: ' + str(extract_confidence('something else @10 @ xxx')))\n","print('test 2: ' + str(extract_confidence('something else @@13@@ xxx')))\n","print('test 3: ' + str(extract_confidence(response)))"]},{"cell_type":"markdown","id":"jDtlTVQ0WDe3","metadata":{"id":"jDtlTVQ0WDe3"},"source":["## Putting everything together\n","Now, you'll run the analysis on the entire data set, i.e., the verbal reports for each decision problem. In each case, the model will assess whether the verbal report reflects the use of a selected reason and return a confidence value.  \n","\n","We have already created the prompts for each verbal report in the chunks above. These prompts stored in the `maximum_outcome_prompts` object were setup to identify the **maximum outcome** reason.  \n","\n","The code below runs these reasons through Llama and stores the results in different ways. The code also defines a few functions that will help us to inspect the results.\n","\n","Running this code will take some time. To keep you updated, the code will print index showing how many of the verbal reports have been processed. Run the code.   \n"]},{"cell_type":"code","execution_count":null,"id":"ufnZmd5pUR1w","metadata":{"id":"ufnZmd5pUR1w"},"outputs":[],"source":["# list for storing the output from the LLAMA model\n","maximum_outcome_eval = []\n","\n","# analyzed reason\n","selected_reason = 'maximum outcome'\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","# remind that selected reason was set to 'maximum outcome'\n","problems_reports[selected_reason] = None\n","\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(maximum_outcome_prompts):\n","\n","    # response from LLAMA\n","    response = LLAMA.text_generation(prompt, max_new_tokens = 1000)\n","    maximum_outcome_eval.append(response) # save the response to the maximum_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, selected_reason] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]))\n","\n","# Function to wrap text\n","def wrap_text(text, width=100):\n","    return \"<br>\".join(textwrap.wrap(text, width))\n","\n","# display data frames in HTML\n","def disp_tab(dd):\n","    dd = dd.map(lambda x: wrap_text(str(x), width=40))\n","    dd = dd.to_html(escape=False)\n","    return display(HTML(dd))\n","\n","# Function to show verbal reports with confidence assesment below and above specified values\n","def show_verbal_reports_in_range(data, reason, min_threshold = 100, max_treshold = 0, show_na = False):\n","\n","    # if true, display the responses for which conf assessemnt couldn't be extracted\n","    if show_na:\n","        filtered_data = data[data[reason] == 999]\n","    else:\n","        filtered_data = data[(data[reason] <= min_threshold) | (data[reason] >= max_treshold) & (data[reason] != 999)]\n","\n","     # wrap the text for nicer display\n","    filtered_data.loc[:, 'verbal_report'] = filtered_data['verbal_report'].apply(wrap_text)\n","    filtered_data.loc[:, 'decision_problem'] = filtered_data['decision_problem'].apply(lambda x: wrap_text(x, width=40))\n","\n","    # select only the columns with report and confidence assesment\n","    filtered_data = filtered_data[['decision_problem', 'verbal_report', 'choice', reason]]\n","    filtered_data = filtered_data.to_html(escape=False) # to html\n","\n","    return display(HTML(filtered_data))"]},{"cell_type":"markdown","id":"POycQ0qnqwQ5","metadata":{"id":"POycQ0qnqwQ5"},"source":["With the function below, you can return verbal reports for which the LLM indicated either an extremely low or high confidence that the **maximum outcome** reason was used. Inspect the verbal reports. Do you agree with the LLMs assessments?"]},{"cell_type":"code","execution_count":null,"id":"-2rC48bLeeph","metadata":{"id":"-2rC48bLeeph"},"outputs":[],"source":["# Show verbal reports for which the Maximum Outcome reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'maximum outcome', min_threshold = 20, max_treshold = 80)"]},{"cell_type":"markdown","id":"IvDLg1NIrGBc","metadata":{"id":"IvDLg1NIrGBc"},"source":["Using the same function, you can view the entire data set. That is, at least in those cases where the LLM provided a confidence value such that it could be properly extracted (i.e., assement between 0 and 100). Take a look at the indicies and see if there are entries missing."]},{"cell_type":"code","execution_count":null,"id":"Vh2srIRoljtj","metadata":{"id":"Vh2srIRoljtj"},"outputs":[],"source":["# Show all results with correct confidence assesments\n","show_verbal_reports_in_range(problems_reports, 'maximum outcome', min_threshold = 100, max_treshold = 0)"]},{"cell_type":"markdown","id":"wMkKU-A-rMyF","metadata":{"id":"wMkKU-A-rMyF"},"source":["If there are entries missing, it can be worthwhile inspecting the complete text output that the LLM provided. Thereby you can assess why the extraction of the confidence value may have failed.\n","\n","To display cases for which confidence extraction didn't work, run the follwing line of code:\n"]},{"cell_type":"code","source":["# Show all results with incorrect confidence assesments\n","show_verbal_reports_in_range(problems_reports, 'maximum outcome', show_na = True)"],"metadata":{"id":"K2-hCVM1Cjwc"},"id":"K2-hCVM1Cjwc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inspecting the full output can also be valuable for other reasons. For instance, the text output may reveal whether the LLM is fully understanding the concepts of your prompt and closely following your instructions.  \n","\n","The complete LLM output was saved in the `maximum_outcome_eval` object. You can access it by printing individual elements. You can select them by their index.\n","\n","Take a look at the full outputs of iterations in which the confidence assessment couldn't be correctly extracted. To do this, pick an index from the table above and replace the `1` in the line below."],"metadata":{"id":"rvalyZuhCXoz"},"id":"rvalyZuhCXoz"},{"cell_type":"code","execution_count":null,"id":"cS9CA-SVraCT","metadata":{"id":"cS9CA-SVraCT"},"outputs":[],"source":["# Print complete LLM output for index 1\n","print(maximum_outcome_eval[1])"]},{"cell_type":"markdown","source":["Finally, you can use `disp_tab` function to display the entire data set"],"metadata":{"id":"DeQnhLlqMJ72"},"id":"DeQnhLlqMJ72"},{"cell_type":"code","source":["disp_tab(problems_reports)"],"metadata":{"id":"5hScbwNcMPF5"},"id":"5hScbwNcMPF5","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"sdlpW7AlmLLQ","metadata":{"id":"sdlpW7AlmLLQ"},"source":["# SURE OUTCOME\n","Run the analyses for the **sure outcome** decision reason. The code below implements the entire pipeline. Run it and see whether LLM identifies cases with low or high confidence. Again, this may take a moment to finish."]},{"cell_type":"code","execution_count":null,"id":"mVnjA_V8mZrS","metadata":{"id":"mVnjA_V8mZrS"},"outputs":[],"source":["# Select the description\n","analyzed_reason = \"sure outcome\"\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == analyzed_reason, 'decision reason description'].values[0]\n","print(selected_description)\n","\n","# Create a list for storing prompts for the sure outcome reason\n","sure_outcome_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the sure outcome reason\n","    prompt = generate_prompt(\n","        prompt_template,\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    sure_outcome_prompts.append(prompt)\n","\n","# Run the LLM\n","# list for storing the output from the LLAMA model\n","sure_outcome_eval = []\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","# remind that selected reason was set to 'sure outcome'\n","problems_reports[analyzed_reason] = None\n","\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(sure_outcome_prompts):\n","\n","    # response from LLAMA\n","    llama_response = LLAMA.text_generation(prompt, max_new_tokens = 4000)\n","    sure_outcome_eval.append(llama_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(llama_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, analyzed_reason] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))\n","\n","# show cases with confidence lower than 20 and higher than 80\n","show_verbal_reports_in_range(problems_reports, 'sure outcome', 20, 80)"]},{"cell_type":"markdown","id":"GMNFQCP-zn6l","metadata":{"id":"GMNFQCP-zn6l"},"source":["# Run other reasons\n","If you have the time, run the code below to review the list of reasons we prepared for you. Select a reason that you like (or don't like) and try to recreate the analysis using this reason. To do this, simply replace\n","\n","Good luck!"]},{"cell_type":"code","execution_count":null,"id":"k5WSvjW5z88U","metadata":{"id":"k5WSvjW5z88U"},"outputs":[],"source":["# display decision reasons\n","disp_tab(decision_reasons)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}