{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979416b3c30fb86b",
   "metadata": {
    "collapsed": false,
    "id": "979416b3c30fb86b",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Using Notebook Environments\n",
    "1. To run a cell, press `shift + enter`. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell (text only), it will render the markdown and move to the next cell.\n",
    "2. Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the kernel by clicking Runtime in the menu bar (if you're using Colab) and selecting `Restart runtime`. This will clear all variables and outputs.\n",
    "3. The final variable in a cell will be printed on the screen. If you want to print multiple variables, use the `print()` function as usual.\n",
    "\n",
    "Notebook environments support code cells and markdown (text) cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with `#`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616368742ccde73",
   "metadata": {
    "collapsed": false,
    "id": "c616368742ccde73",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Environment Setup\n",
    "The code below allows the notebook to acces your google files, including reading the files in, and writing files in specified locations.\n",
    "Once you run it, you'll have to give the notebbok the access to a google drive of your choosen google account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tcx3KlNmeU3H",
   "metadata": {
    "id": "tcx3KlNmeU3H"
   },
   "outputs": [],
   "source": [
    "# mount googl drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c24c2ed829f80",
   "metadata": {
    "collapsed": false,
    "id": "290c24c2ed829f80",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the `import <name> as <nickname>` syntax (e.g. `import pandas as pd`). These are usually standardized nicknames.\n",
    "\n",
    "We'll use following packages:\n",
    "\n",
    "1. `pandas`: A very popular package for reading and manipulating data in python.\n",
    "2. `re` for string matching\n",
    "3. `textwrap` and `IPython` to make the display of some of the tables more readable\n",
    "4. `pandas` is a popular package for data frames manipulation\n",
    "5. `huggingface_hub` for making API calls and prompt the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# the code below installs huggingface hub if it's missing\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "\n",
    "    # Installing requisite packages\n",
    "    !pip install huggingface_hub &> /dev/null\n",
    "\n",
    "# this sets the working directory to the exercises folder\n",
    "base_path = '/content/drive/My Drive/llms_egproc/exercises/'\n",
    "os.chdir(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df0bbd",
   "metadata": {
    "id": "b8df0bbd"
   },
   "source": [
    "# Identifying decision reasons\n",
    "In this exercise, we will explore the capabilities of LLMs to identify decision reasons in verbal reports using the Hugging Face (HF) ecosystem.\n",
    "\n",
    "By the end of this exercise, you will have learned how to:\n",
    "- Design a zero-shot prompt\n",
    "- Have large models on the Hugging Face servers evaluate your prompts  \n",
    "- Validate the model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf097acf12ea11",
   "metadata": {
    "collapsed": false,
    "id": "f6bf097acf12ea11",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Getting access to a Large Language Model\n",
    "Powerful LLMs like LLAMA-3-70b and even its smaller siblings are typically too large to download and run on your local machine. One alternative solution to get access to LLMs is to use APIs that run the LLMs on remote servers. In this exercise, we will use the API provided by Hugging Face, which is the central actor in the open language model sphere. \n",
    "\n",
    "In addition to offering hundreds of thousands of LLMs, including the most powerful open models developed by Meta, Google, and others, Hugging Face offers a collection of Python libraries to facilitate the use of open LLMs. We will make use of the `huggingface_hub` library to interact with the models running on Hugging Face's servers.  \n",
    "\n",
    "We start by setting up the API `InferenceClient` which we have loaded from the `huggingface_hub` library in the previous chunk. The function takes two argumentsare:\n",
    "* `model`: the model name (see, e.g., https://huggingface.co/meta-llama for collection of Llama models)\n",
    "* `token`: your personal access token obtained through your Hugging Face account.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HlFQGLUDi_gD",
   "metadata": {
    "id": "HlFQGLUDi_gD"
   },
   "outputs": [],
   "source": [
    "# paste your token here\n",
    "API_TOKEN = ''\n",
    "\n",
    "# we'll use latest, small LLAMA-3 model\n",
    "LLAMA_version = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# pass model version and the token to the InferenceClient function and save the output under some name, e.g., LLAMA\n",
    "LLAMA = InferenceClient(model = LLAMA_version, token = API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zc33BH8bwWR-",
   "metadata": {
    "id": "zc33BH8bwWR-"
   },
   "source": [
    "## Using the InferenceClient\n",
    "Now, you can use `.text_generation` method of the `LLAMA` object to prompt the model. It takes the prompt and a `max_new_tokens` argument controlling the size of the text output as inputs. \n",
    "1. run the chunk and investigate the output.\n",
    "2. try other prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jgdi8rXOjUHl",
   "metadata": {
    "id": "Jgdi8rXOjUHl"
   },
   "outputs": [],
   "source": [
    "# let's create some prompt\n",
    "prompt = 'Once upon a time...'\n",
    "\n",
    "# Get a response from the Meta-Llama-3.1-8B-Instruct\n",
    "response = LLAMA.text_generation(prompt = stupid_prompt, max_new_tokens = 300)\n",
    "print(stupid_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5R0AEL-Jwn__",
   "metadata": {
    "id": "5R0AEL-Jwn__"
   },
   "source": [
    "Being an assistant model, Llama distinguishes between system and user messages of the prompt. The two are separated using a collection of special tokens. The code below shows an example with special tokens. Run the prompt as is and study the result. Then change the system role to one of the following and run again: \n",
    "*   Priest\n",
    "*   Economist\n",
    "*   Generic grandpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_l7f3Avpkiv_",
   "metadata": {
    "id": "_l7f3Avpkiv_"
   },
   "outputs": [],
   "source": [
    "# system message sets the role to a decision scientis\n",
    "prompt = \"\"\"\n",
    "<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>\n",
    "You are an expert decision scientist\n",
    "<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What is the best way to make financial decisions?\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# run prompt\n",
    "response = LLAMA.text_generation(full_prompt, max_new_tokens = 200)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8d5804ede6511",
   "metadata": {
    "collapsed": false,
    "id": "efc8d5804ede6511",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Piecing the prompt together\n",
    "In this section, we will build a prompt for reason identification. Let's first take a look at our `prompt_template`. Read it from your google drive and study its individual components. Which prompting techniques do you see at work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb44bcf-bb8a-4603-8c25-33b1537399e6",
   "metadata": {
    "collapsed": true,
    "id": "fbb44bcf-bb8a-4603-8c25-33b1537399e6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read the File\n",
    "prompt_path = 'prompts/prompt_v1.txt'\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(prompt_path, 'r') as file:\n",
    "    prompt_template = file.read()\n",
    "\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830e24a5d45430d",
   "metadata": {
    "collapsed": false,
    "id": "4830e24a5d45430d",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "To complete the prompt, the following three pieces of information need to be replaced for each trial. This information will come from files on your drive. \n",
    "\n",
    "1. The DECISION REASON\n",
    "2. The DECISION PROBLEM\n",
    "3. The VERBAL REPORT\n",
    "\n",
    "The code below reads the files with this information and specifies a function that will help us fill in this information efficiently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LTbks_7t0_fI",
   "metadata": {
    "id": "LTbks_7t0_fI"
   },
   "outputs": [],
   "source": [
    "# read in decision problems, decision reasons, and verbal reports\n",
    "decision_problems = pd.read_csv('data/decision_problems.csv', encoding = 'utf-8')\n",
    "decision_reasons = pd.read_csv('data/decision_reasons.csv', encoding = 'utf-8')\n",
    "verbal_reports = pd.read_csv('data/verbal_reports.csv', encoding = 'utf-8')\n",
    "\n",
    "# merge verbal reports with decision problems\n",
    "problems_reports = pd.merge(decision_problems, verbal_reports, on = 'problem_id')\n",
    "\n",
    "# function for constructing the full prompt\n",
    "def generate_prompt(prompt_template, decision_problem, decision_reason, verbal_report):\n",
    "    \"\"\"\n",
    "    Replaces placeholders in the prompt with the given decision problem, decision reason, and verbal report.\n",
    "    \"\"\"\n",
    "    # Replace placeholders with actual values\n",
    "    full_prompt = prompt_template.replace(\"DECISION_PROBLEM\", decision_problem)\n",
    "    full_prompt = full_prompt.replace(\"DECISION_REASON\", decision_reason)\n",
    "    full_prompt = full_prompt.replace(\"VERBAL_REPORT\", verbal_report)\n",
    "\n",
    "    return full_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Sdhzqhl5_rV2",
   "metadata": {
    "id": "Sdhzqhl5_rV2"
   },
   "source": [
    "Now, let's complete the first prompt. Use the code below to create the first set of prompts for the maximum outcome decision reason. Run code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C-_hO7wnHgsj",
   "metadata": {
    "id": "C-_hO7wnHgsj"
   },
   "outputs": [],
   "source": [
    "# Select the description for expected value\n",
    "selected_reason = 'maximum outcome'\n",
    "\n",
    "# get the description of the reason from the decision_reasons data frame\n",
    "selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]\n",
    "\n",
    "# Create a list for storing prompts for the expected value reason\n",
    "maximum_outcome_prompts = []\n",
    "\n",
    "# Generate prompts for the specific decision reason\n",
    "# this loops over each row of data with verbal reports and corrsponding decision problems\n",
    "for _, row in problems_reports.iterrows():\n",
    "\n",
    "    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n",
    "    prompt = generate_prompt(\n",
    "        prompt_template,\n",
    "        row['decision_problem'],\n",
    "        selected_description,  # Use the selected description\n",
    "        row['verbal_report']\n",
    "    )\n",
    "    maximum_outcome_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5281cd68-52be-4942-893b-62e534a7f986",
   "metadata": {},
   "source": [
    "Have a look at the first full prompt. You can change the index to inspect prompts for other problems and reports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MnKgi_FiKt4r",
   "metadata": {
    "id": "MnKgi_FiKt4r"
   },
   "outputs": [],
   "source": [
    "# you can investigate other prompts by changing the number from 0 to some other value\n",
    "print(maximum_outcome_prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81690eb2b6590715",
   "metadata": {
    "collapsed": false,
    "id": "81690eb2b6590715",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Testing the identification of reasons\n",
    "\n",
    "In this section, we will try out identifying reasons using the LLM. The code below sends the first maximum outcome prompt to the LLM. Run it and make sense of the output relative to the prompt. You can again try other prompts by changing the the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bDvebdMDM9I1",
   "metadata": {
    "id": "bDvebdMDM9I1"
   },
   "outputs": [],
   "source": [
    "# pass the first prompt to LLAMA and save the output\n",
    "result = LLAMA.text_generation(maximum_outcome_prompts[0], max_new_tokens = 1000)\n",
    "\n",
    "# print the llama evaluation\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bLQ95iRBP798",
   "metadata": {
    "id": "bLQ95iRBP798"
   },
   "source": [
    "You may have picked that our prompt instructs you to return the confidence for the presence of the reason in a particular format. We do this so that it is easy to extract the confidence value from the output text. The code below defines a simple function to extract the confidence and applies it to a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qD2itiT6RkvJ",
   "metadata": {
    "id": "qD2itiT6RkvJ"
   },
   "outputs": [],
   "source": [
    "# Function for extracting confidence assessments\n",
    "def extract_confidence(s):\n",
    "    \"\"\"\n",
    "    Extracts an integer value from a string enclosed between @ or @@ symbols.\n",
    "    \"\"\"\n",
    "    # Regular expression to match patterns like @number@ or @@number@@\n",
    "    pattern = r'@+(\\s*\\d+\\s*)@+'\n",
    "\n",
    "    # Search for the pattern in the string\n",
    "    match = re.search(pattern, s)\n",
    "\n",
    "    if match:\n",
    "        # Extract the number and convert it to an integer\n",
    "        number_str = match.group(1).strip()\n",
    "        return int(number_str)\n",
    "\n",
    "    return None\n",
    "\n",
    "# test the extract_confidence function\n",
    "print(extract_confidence('something else @10 @ xxx'))\n",
    "print(extract_confidence('something else @@13@@ xxx'))\n",
    "print(extract_confidence(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jDtlTVQ0WDe3",
   "metadata": {
    "id": "jDtlTVQ0WDe3"
   },
   "source": [
    "## Putting everything together\n",
    "Now you'll run the analysis on the etire data set. For each verbal report and decision problem combination, the model with provide assessment of confidence on whether the individual used the expected value reason.\n",
    "\n",
    "We will iterate over the entier list with prompts containinig the **maximum outcome** reason (stored in the `maximum_outcome_prompts`). On each iteration the model will assess if **maximum outcome*** reason was used by the individual, based on the verbal report.\n",
    "\n",
    "The functions below are not too imporant. Their only goal is to display the tables in the notebbok in a nice, HTML format, which is easier to read than the base output of `print()`.\n",
    "\n",
    "Specifically:\n",
    "1. We pass a `prompt` to the model\n",
    "2. The full output from the LLAMA , `llama_response` is saved in a list `maximum_outcome_eval` for later inspection\n",
    "3. We use the `extract_confidence` function to extract the confidence assesment from `llama_response`\n",
    "4. We save the `confidence_assesment` in the new colmun of the data set with decision problems abnd verbal reports `problems_reports['maximum outcome']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufnZmd5pUR1w",
   "metadata": {
    "id": "ufnZmd5pUR1w"
   },
   "outputs": [],
   "source": [
    "# Function to wrap text\n",
    "def wrap_text(text, width=100):\n",
    "    return \"<br>\".join(textwrap.wrap(text, width))\n",
    "\n",
    "# display data frames in HTML\n",
    "def disp_tab(dd):\n",
    "    dd = dd.to_html(escape=False)\n",
    "    return display(HTML(dd))\n",
    "\n",
    "# Function to show verbal reports with assigned numbers in a specified range\n",
    "def show_verbal_reports_in_range(data, reason, min_confidence, max_confidence):\n",
    "    \"\"\"\n",
    "    Shows verbal reports for which the model assigned a confidence within the specified range.\n",
    "    \"\"\"\n",
    "    filtered_data = data[(data[reason] >= min_confidence) & (data[reason] <= max_confidence)] # filter by the specified range\n",
    "\n",
    "     # wrap the text for nicer display\n",
    "    filtered_data.loc[:, 'verbal_report'] = filtered_data['verbal_report'].apply(wrap_text)\n",
    "    filtered_data.loc[:, 'decision_problem'] = filtered_data['decision_problem'].apply(lambda x: wrap_text(x, width=40))\n",
    "\n",
    "    # select only the columns with report and confidence assesment\n",
    "    filtered_data = filtered_data[['decision_problem', 'verbal_report', 'choice', reason]]\n",
    "    filtered_data = filtered_data.to_html(escape=False) # to html\n",
    "\n",
    "    return display(HTML(filtered_data))\n",
    "    # return filtered_data[['verbal_report', reason]]\n",
    "\n",
    "# list for storing the output from the LLAMA model\n",
    "maximum_outcome_eval = []\n",
    "\n",
    "# analyzed reason\n",
    "analyzed_reason = \"maximum outcome\"\n",
    "\n",
    "# new column in the problems_reports data set for stroting the confidence assesments\n",
    "# remind that selected reason was set to 'expected value'\n",
    "problems_reports[analyzed_reason] = None\n",
    "\n",
    "# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n",
    "for i, prompt in enumerate(maximum_outcome_prompts):\n",
    "\n",
    "    # response from LLAMA\n",
    "    llama_response = LLAMA.text_generation(prompt, max_new_tokens = 4000)\n",
    "    maximum_outcome_eval.append(llama_response) # save the response to the expected_value_eval list\n",
    "\n",
    "    # extract the confidence value from the response\n",
    "    confidence_assesment = extract_confidence(llama_response)\n",
    "\n",
    "    # confidence value into the data\n",
    "    problems_reports.at[i, analyzed_reason] = confidence_assesment\n",
    "\n",
    "    # monitor progress\n",
    "    print(str(i) + '/' + str(problems_reports.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "POycQ0qnqwQ5",
   "metadata": {
    "id": "POycQ0qnqwQ5"
   },
   "source": [
    "#### High confidence assesments\n",
    "The code below displays the verbal reports for which the LLM thought that there is a HIGH chance that the **maximum outcome** reason was used when making the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-2rC48bLeeph",
   "metadata": {
    "id": "-2rC48bLeeph"
   },
   "outputs": [],
   "source": [
    "# Show verbal reports for which the Expected value reason was assessed to be used with high confidecne, i.e., between 80 to 100\n",
    "show_verbal_reports_in_range(problems_reports, 'maximum outcome', 80, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bB7FdZuFq_sG",
   "metadata": {
    "id": "bB7FdZuFq_sG"
   },
   "source": [
    "#### Low confidence assesments\n",
    "The code below displays the verbal reports for which the LLM thought that there is a LOW chance that the **maximum outcome** reason was used when making the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y2Vz_ETwewnT",
   "metadata": {
    "id": "Y2Vz_ETwewnT"
   },
   "outputs": [],
   "source": [
    "# Show verbal reports for which the Expected value reason was assessed not to be used---i.e., confidence in usuing th reason was low, between 0 and 20\n",
    "show_verbal_reports_in_range(problems_reports, 'maximum outcome', 0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IvDLg1NIrGBc",
   "metadata": {
    "id": "IvDLg1NIrGBc"
   },
   "source": [
    "#### Full data\n",
    "You can view the entire data set by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vh2srIRoljtj",
   "metadata": {
    "id": "Vh2srIRoljtj"
   },
   "outputs": [],
   "source": [
    "# Show all results\n",
    "show_verbal_reports_in_range(problems_reports, analyzed_reason, 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wMkKU-A-rMyF",
   "metadata": {
    "id": "wMkKU-A-rMyF"
   },
   "source": [
    "#### LLM reasoninig\n",
    "Full LLM output was saved in the `maximum_outcome_eval` object. You can acces it by printing the elements one at a time. Notice that the output tables above provide row numbers in the leftmost column. These numbers corresponds to the entries in the `maximum_outcome_eval` object.\n",
    "\n",
    "Thus, if you want to display the LLM deliberatino process from which the assesment in row 1 was taken, you simply run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cS9CA-SVraCT",
   "metadata": {
    "id": "cS9CA-SVraCT"
   },
   "outputs": [],
   "source": [
    "print(maximum_outcome_eval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sdlpW7AlmLLQ",
   "metadata": {
    "id": "sdlpW7AlmLLQ"
   },
   "source": [
    "# SURE OUTCOME\n",
    "Run the analyses for the **sure outcome** decision reason.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WsK2JWigs7v5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 300,
     "status": "ok",
     "timestamp": 1722260801869,
     "user": {
      "displayName": "Kamil FuÅ‚awka",
      "userId": "16000584420062196934"
     },
     "user_tz": -120
    },
    "id": "WsK2JWigs7v5",
    "outputId": "8fc795a7-3f77-4a63-d01c-d38040ab9742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The reason considers the presence of a sure outcome, that is an outcome with 100% probability, of each lottery. The reason prefers the lottery with or without the sure outcome, depending on whether the sure outcome is a favorable outcome in the context of all possible outcomes.\n"
     ]
    }
   ],
   "source": [
    "# Select the description for expected value\n",
    "analyzed_reason = \"sure outcome\"\n",
    "selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == analyzed_reason, 'decision reason description'].values[0]\n",
    "print(selected_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YTOuqci6nAbw",
   "metadata": {
    "id": "YTOuqci6nAbw"
   },
   "source": [
    "Create the list with prompts contatinig **sure outcome** reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mVnjA_V8mZrS",
   "metadata": {
    "id": "mVnjA_V8mZrS"
   },
   "outputs": [],
   "source": [
    "# Create a list for storing prompts for the expected value reason\n",
    "sure_outcome_prompts = []\n",
    "\n",
    "# Generate prompts for the specific decision reason\n",
    "for _, row in problems_reports.iterrows():\n",
    "\n",
    "    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n",
    "    prompt = generate_prompt(\n",
    "        prompt_template,\n",
    "        row['decision_problem'],\n",
    "        selected_description,  # Use the selected description\n",
    "        row['verbal_report']\n",
    "    )\n",
    "    sure_outcome_prompts.append(prompt)\n",
    "\n",
    "print(sure_outcome_prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A00i1bzuvwel",
   "metadata": {
    "id": "A00i1bzuvwel"
   },
   "source": [
    "Run the LLM on a random prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AfPOIIyev4AZ",
   "metadata": {
    "id": "AfPOIIyev4AZ"
   },
   "outputs": [],
   "source": [
    "sure_outcome_res = LLAMA.text_generation(prompt = sure_outcome_prompts[5], max_new_tokens=4000),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1FfRpG_wXY_",
   "metadata": {
    "id": "n1FfRpG_wXY_"
   },
   "outputs": [],
   "source": [
    "print(sure_outcome_res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kC6KPO2pyt8d",
   "metadata": {
    "id": "kC6KPO2pyt8d"
   },
   "source": [
    "Run the LLM on the entire list of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FLk9D_u-nIZB",
   "metadata": {
    "id": "FLk9D_u-nIZB"
   },
   "outputs": [],
   "source": [
    "# list for storing the output from the LLAMA model\n",
    "sure_outcome_eval = []\n",
    "\n",
    "# new column in the problems_reports data set for stroting the confidence assesments\n",
    "# remind that selected reason was set to 'expected value'\n",
    "problems_reports[analyzed_reason] = None\n",
    "\n",
    "# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n",
    "for i, prompt in enumerate(sure_outcome_prompts):\n",
    "\n",
    "    # response from LLAMA\n",
    "    llama_response = LLAMA.text_generation(prompt, max_new_tokens = 4000)\n",
    "    sure_outcome_eval.append(llama_response) # save the response to the sure_outcome_eval list\n",
    "\n",
    "    # extract the confidence value from the response\n",
    "    confidence_assesment = extract_confidence(llama_response)\n",
    "\n",
    "    # confidence value into the data\n",
    "    problems_reports.at[i, analyzed_reason] = confidence_assesment\n",
    "\n",
    "    # monitor progress\n",
    "    print(str(i) + '/' + str(problems_reports.shape[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UQjoJc3Ty37Z",
   "metadata": {
    "id": "UQjoJc3Ty37Z"
   },
   "source": [
    "High confidence assesments for sure outcome reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4mDf7-ZZn7Bs",
   "metadata": {
    "id": "4mDf7-ZZn7Bs"
   },
   "outputs": [],
   "source": [
    "# Show verbal reports for which the Sure outcome reason was assessed to be used with high confidecne, i.e., between 80 to 100\n",
    "show_verbal_reports_in_range(problems_reports, 'sure outcome', 80, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0AokAxEMzGnc",
   "metadata": {
    "id": "0AokAxEMzGnc"
   },
   "source": [
    "Low confidence assesments for sure outcome reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fGK3fCVSnuPD",
   "metadata": {
    "id": "fGK3fCVSnuPD"
   },
   "outputs": [],
   "source": [
    "# Show verbal reports for which the Sure outcome reason was assessed not to be used---i.e., confidence in usuing th reason was low, between 0 and 20\n",
    "show_verbal_reports_in_range(problems_reports, 'sure outcome', 0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ycVQ6FhvzTnZ",
   "metadata": {
    "id": "ycVQ6FhvzTnZ"
   },
   "source": [
    "The full LLM reasoninig for selected data point\n",
    "Change the number to view reasoninig associated with the data point of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SVpHFAVUzb6K",
   "metadata": {
    "id": "SVpHFAVUzb6K"
   },
   "outputs": [],
   "source": [
    "print(maximum_outcome_eval[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GMNFQCP-zn6l",
   "metadata": {
    "id": "GMNFQCP-zn6l"
   },
   "source": [
    "# Recrating the analysis on your own\n",
    "If you have the time, have a look at the list of reasons we prepared for you. Selecet a reason that you like (or don't like) and try to recrated the analysis for it.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k5WSvjW5z88U",
   "metadata": {
    "id": "k5WSvjW5z88U"
   },
   "outputs": [],
   "source": [
    "# display decision reasons\n",
    "disp_tab(decision_reasons)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
