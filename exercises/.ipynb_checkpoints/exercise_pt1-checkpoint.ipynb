{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979416b3c30fb86b",
   "metadata": {
    "collapsed": false,
    "id": "979416b3c30fb86b",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Using Notebook Environments\n",
    "1. To run a cell, press `shift + enter`. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell (text only), it will render the markdown and move to the next cell.\n",
    "2. Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the kernel by clicking Runtime in the menu bar (if you're using Colab) and selecting `Restart runtime`. This will clear all variables and outputs.\n",
    "3. The final variable in a cell will be printed on the screen. If you want to print multiple variables, use the `print()` function as usual.\n",
    "\n",
    "Notebook environments support code cells and markdown (text) cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with `#`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616368742ccde73",
   "metadata": {
    "collapsed": false,
    "id": "c616368742ccde73",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "tcx3KlNmeU3H",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21800,
     "status": "ok",
     "timestamp": 1722076341684,
     "user": {
      "displayName": "Kamil Fuławka",
      "userId": "16000584420062196934"
     },
     "user_tz": -120
    },
    "id": "tcx3KlNmeU3H",
    "outputId": "815fa2d5-4415-485a-86ff-9d5fb656f84a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# mount googl drive\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# mount googl drive\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c24c2ed829f80",
   "metadata": {
    "collapsed": false,
    "id": "290c24c2ed829f80",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the `import <name> as <nickname>` syntax (e.g. `import pandas as pd`). These are usually standardized nicknames.\n",
    "\n",
    "We'll use following packages:\n",
    "\n",
    "1. `pandas`: A very popular package for reading and manipulating data in python.\n",
    "2. `huggingface_hub`: A package for extracting features from text data using\n",
    "3. `re` for string matching\n",
    "4. `textwrap` and `IPython` to make the display of some of the tables more readable\n",
    "5. `pandas` is a popular package for data frames manipulation\n",
    "6. `huggingface_hub` for making API calls and prompt the LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 6156,
     "status": "ok",
     "timestamp": 1722076350090,
     "user": {
      "displayName": "Kamil Fuławka",
      "userId": "16000584420062196934"
     },
     "user_tz": -120
    },
    "id": "initial_id",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# the code below installs huggingface hub if it's missing\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "\n",
    "    # Installing requisite packages\n",
    "    !pip install huggingface_hub &> /dev/null\n",
    "\n",
    "# this sets the working directory to the exercises folder\n",
    "base_path = '/content/drive/My Drive/llms_egproc/exercises/'\n",
    "os.chdir(base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8df0bbd",
   "metadata": {
    "id": "b8df0bbd"
   },
   "source": [
    "# Identifying decision reasons (part 1)\n",
    "In this exercise, we will explore the capabilities of LLMs to identify decision reasons in verbal reports using the Hugging Face (HF) ecosystem.\n",
    "\n",
    "By the end of this exercise, you will have learned how to:\n",
    "- Design a zero-shot prompt\n",
    "- Have large models on the Hugging Face servers evaluate your prompts  \n",
    "- Validate the model output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf097acf12ea11",
   "metadata": {
    "collapsed": false,
    "id": "f6bf097acf12ea11",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Hello world\n",
    "We start by settiing up the inference client. The `InferenceClient` function from huggingface ecosystem allows for....\n",
    "The API_TOKEN is an atuhetication token...\n",
    "The LLAMA_version variable specifies the version of the LLAMA model that will use in this exercise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "HlFQGLUDi_gD",
   "metadata": {
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1722076365925,
     "user": {
      "displayName": "Kamil Fuławka",
      "userId": "16000584420062196934"
     },
     "user_tz": -120
    },
    "id": "HlFQGLUDi_gD"
   },
   "outputs": [],
   "source": [
    "API_TOKEN = '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'\n",
    "LLAMA_version = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n",
    "LLAMA = InferenceClient(model = LLAMA_version, token = API_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zc33BH8bwWR-",
   "metadata": {
    "id": "zc33BH8bwWR-"
   },
   "source": [
    "Try out some stupid prompt. Note that in order to prompt the model the `.text_generation` method should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jgdi8rXOjUHl",
   "metadata": {
    "id": "Jgdi8rXOjUHl"
   },
   "outputs": [],
   "source": [
    "stupid_prompt = 'Give an example of a stupid prompt'\n",
    "llama_response = LLAMA.text_generation(stupid_prompt, max_new_tokens = 4000)\n",
    "print(llama_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5R0AEL-Jwn__",
   "metadata": {
    "id": "5R0AEL-Jwn__"
   },
   "source": [
    "## System and User messages\n",
    "LLAMA uses special tokens to distinguish between system and user parts of the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_l7f3Avpkiv_",
   "metadata": {
    "id": "_l7f3Avpkiv_"
   },
   "outputs": [],
   "source": [
    "system_role = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are an expert decision scientist\n",
    "<|eot_id|>\"\"\"\n",
    "\n",
    "user_question = \"\"\"\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What is the best way to make financial decisions?\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# combine both into a single prompt\n",
    "prompt = system_role + user_question\n",
    "\n",
    "# print the full prompt\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sld79yiZw8cy",
   "metadata": {
    "id": "sld79yiZw8cy"
   },
   "source": [
    "Try the prompt with the model role set to decision scientist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1SMSSFtMmP48",
   "metadata": {
    "id": "1SMSSFtMmP48"
   },
   "outputs": [],
   "source": [
    "llama_response = LLAMA.text_generation(prompt, max_new_tokens = 200)\n",
    "print(llama_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fZgdD9h-xCEs",
   "metadata": {
    "id": "fZgdD9h-xCEs"
   },
   "source": [
    "Now try it with the role set to a priest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Re-Vtw3cxG-_",
   "metadata": {
    "id": "Re-Vtw3cxG-_"
   },
   "outputs": [],
   "source": [
    "# change the role in line and rerun the cell\n",
    "\n",
    "system_role = \"\"\"\n",
    "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "You are a priest\n",
    "<|eot_id|>\"\"\"\n",
    "\n",
    "user_question = \"\"\"\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "What is the best way to make financial decisions?\n",
    "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
    "\n",
    "# update the prompt\n",
    "prompt = system_role + user_question\n",
    "\n",
    "llama_response = LLAMA.text_generation(prompt, max_new_tokens = 200)\n",
    "print(llama_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8d5804ede6511",
   "metadata": {
    "collapsed": false,
    "id": "efc8d5804ede6511",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Loading the prompt template\n",
    "Now have a look at our prompt. What kind of information do we provide in the system message?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb44bcf-bb8a-4603-8c25-33b1537399e6",
   "metadata": {
    "collapsed": true,
    "id": "fbb44bcf-bb8a-4603-8c25-33b1537399e6",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Read the File\n",
    "prompt_path = 'prompts/prompt_v1.txt'\n",
    "\n",
    "# Open the file and read its contents\n",
    "with open(prompt_path, 'r') as file:\n",
    "    prompt_base = file.read()\n",
    "\n",
    "print(prompt_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830e24a5d45430d",
   "metadata": {
    "collapsed": false,
    "id": "4830e24a5d45430d",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Constructing the prompt\n",
    "\n",
    "- read in the decision problems and reports\n",
    "- combine them to vector of complete prompts\n",
    "- look at 1 complete prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LTbks_7t0_fI",
   "metadata": {
    "id": "LTbks_7t0_fI"
   },
   "outputs": [],
   "source": [
    "# read in decision problems, decision reasons, and verbal reports\n",
    "decision_problems = pd.read_csv('data/decision_problems.csv', encoding = 'utf-8')\n",
    "decision_reasons = pd.read_csv('data/decision_reasons.csv', encoding = 'utf-8')\n",
    "verbal_reports = pd.read_csv('data/verbal_reports.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_SrXp2H-EtKN",
   "metadata": {
    "id": "_SrXp2H-EtKN"
   },
   "outputs": [],
   "source": [
    "# merge verbal reports with decision problems\n",
    "problems_reports = pd.merge(decision_problems, verbal_reports, on = 'problem_id')\n",
    "# print(problems_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T2KwSGnxFBCX",
   "metadata": {
    "id": "T2KwSGnxFBCX"
   },
   "outputs": [],
   "source": [
    "# print(decision_reasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yA7j1zok7pl_",
   "metadata": {
    "id": "yA7j1zok7pl_"
   },
   "outputs": [],
   "source": [
    "# function for constructing the full prompt\n",
    "def generate_prompt(prompt, decision_problem, decision_reason, verbal_report):\n",
    "    \"\"\"\n",
    "    Replaces placeholders in the prompt with the given decision problem, decision reason, and verbal report.\n",
    "    \"\"\"\n",
    "    # Replace placeholders with actual values\n",
    "    filled_prompt = prompt.replace(\"DECISION_PROBLEM\", decision_problem)\n",
    "    filled_prompt = filled_prompt.replace(\"DECISION_REASON\", decision_reason)\n",
    "    filled_prompt = filled_prompt.replace(\"VERBAL_REPORT\", verbal_report)\n",
    "\n",
    "    return filled_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "C-_hO7wnHgsj",
   "metadata": {
    "id": "C-_hO7wnHgsj"
   },
   "outputs": [],
   "source": [
    "# Select the description for expected value\n",
    "selected_reason = 'expected value'\n",
    "selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]\n",
    "\n",
    "# Create a list for storing prompts for the expected value reason\n",
    "expected_value_prompts = []\n",
    "\n",
    "# Generate prompts for the specific decision reason\n",
    "for _, row in problems_reports.iterrows():\n",
    "\n",
    "    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n",
    "    prompt = generate_prompt(\n",
    "        prompt_base,\n",
    "        row['decision_problem'],\n",
    "        selected_description,  # Use the selected description\n",
    "        row['verbal_report']\n",
    "    )\n",
    "    expected_value_prompts.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MnKgi_FiKt4r",
   "metadata": {
    "id": "MnKgi_FiKt4r"
   },
   "outputs": [],
   "source": [
    "# have a look at first\n",
    "print(expected_value_prompts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81690eb2b6590715",
   "metadata": {
    "collapsed": false,
    "id": "81690eb2b6590715",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Give it a test\n",
    "\n",
    "- run first prompt\n",
    "- make sense of output relative to prompt\n",
    "- maybe try again or next prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bDvebdMDM9I1",
   "metadata": {
    "id": "bDvebdMDM9I1"
   },
   "outputs": [],
   "source": [
    "# pass the first prompt to LLAMA and save the output\n",
    "expected_value_eval1 = LLAMA.text_generation(expected_value_prompts[1], max_new_tokens = 4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wUZDnpOdNTKd",
   "metadata": {
    "id": "wUZDnpOdNTKd"
   },
   "outputs": [],
   "source": [
    "# print the llama evaluation\n",
    "print(expected_value_eval1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bLQ95iRBP798",
   "metadata": {
    "id": "bLQ95iRBP798"
   },
   "source": [
    "# Extractig confidence ratings\n",
    "As you can see in the output above, following our prompt the model provides full descrrpition of the deliberation process and the confidence assesment at the end.\n",
    "\n",
    "Next, we will iterate over the entier data set. While doing so, we will use the `extract_confidence` function to extract the confidence assesment and append it to the `problems_reports` data set. The full output from the LLAMA model will be saved in a separate list `expected_value_eval` for later inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qD2itiT6RkvJ",
   "metadata": {
    "id": "qD2itiT6RkvJ"
   },
   "outputs": [],
   "source": [
    "# Function for extracting confidence assessments\n",
    "def extract_confidence(s):\n",
    "    \"\"\"\n",
    "    Extracts an integer value from a string enclosed between @ or @@ symbols.\n",
    "    \"\"\"\n",
    "    # Regular expression to match patterns like @number@ or @@number@@\n",
    "    pattern = r'@+(\\s*\\d+\\s*)@+'\n",
    "\n",
    "    # Search for the pattern in the string\n",
    "    match = re.search(pattern, s)\n",
    "\n",
    "    if match:\n",
    "        # Extract the number and convert it to an integer\n",
    "        number_str = match.group(1).strip()\n",
    "        return int(number_str)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FelpKr90WFhK",
   "metadata": {
    "id": "FelpKr90WFhK"
   },
   "outputs": [],
   "source": [
    "# # test the extract_confidence function\n",
    "# print(extract_confidence('something else @10 @ xxx'))\n",
    "# print(extract_confidence('something else @@13@@ xxx'))\n",
    "# print(extract_confidence(expected_value_eval1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jDtlTVQ0WDe3",
   "metadata": {
    "id": "jDtlTVQ0WDe3"
   },
   "source": [
    "# EXPECTED VALUE\n",
    "## Example Analysis\n",
    "Now you'll run the analysis on the etire data set. For each verbal report and decision problem combination, the model with provide assessment of confidence on whether the individual used the expected value reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ufnZmd5pUR1w",
   "metadata": {
    "id": "ufnZmd5pUR1w"
   },
   "outputs": [],
   "source": [
    "# list for storing the output from the LLAMA model\n",
    "expected_value_eval = []\n",
    "\n",
    "# analyzed reason\n",
    "analyzed_reason = \"expected value\"\n",
    "\n",
    "# new column in the problems_reports data set for stroting the confidence assesments\n",
    "# remind that selected reason was set to 'expected value'\n",
    "problems_reports[analyzed_reason] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YJ5QQTc1Vexx",
   "metadata": {
    "id": "YJ5QQTc1Vexx"
   },
   "outputs": [],
   "source": [
    "# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n",
    "for i, prompt in enumerate(expected_value_prompts):\n",
    "\n",
    "    # response from LLAMA\n",
    "    llama_response = LLAMA.text_generation(prompt, max_new_tokens = 4000)\n",
    "    expected_value_eval.append(llama_response) # save the response to the expected_value_eval list\n",
    "\n",
    "    # extract the confidence value from the response\n",
    "    confidence_assesment = extract_confidence(llama_response)\n",
    "\n",
    "    # confidence value into the data\n",
    "    problems_reports.at[i, analyzed_reason] = confidence_assesment\n",
    "\n",
    "    # monitor progress\n",
    "    print(str(i) + '/' + str(problems_reports.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YE4WDuE3Z5Iv",
   "metadata": {
    "id": "YE4WDuE3Z5Iv"
   },
   "outputs": [],
   "source": [
    "# Function to wrap text\n",
    "def wrap_text(text, width=100):\n",
    "    return \"<br>\".join(textwrap.wrap(text, width))\n",
    "\n",
    "# Function to show verbal reports with assigned numbers in a specified range\n",
    "def show_verbal_reports_in_range(data, reason, min_confidence, max_confidence):\n",
    "    \"\"\"\n",
    "    Shows verbal reports for which the model assigned a confidence within the specified range.\n",
    "    \"\"\"\n",
    "    filtered_data = data[(data[reason] >= min_confidence) & (data[reason] <= max_confidence)] # filter by the specified range\n",
    "\n",
    "     # wrap the text for nicer display\n",
    "    filtered_data.loc[:, 'verbal_report'] = filtered_data['verbal_report'].apply(wrap_text)\n",
    "    filtered_data.loc[:, 'decision_problem'] = filtered_data['decision_problem'].apply(lambda x: wrap_text(x, width=40))\n",
    "\n",
    "    # select only the columns with report and confidence assesment\n",
    "    filtered_data = filtered_data[['decision_problem', 'verbal_report', 'choice', reason]]\n",
    "    filtered_data = filtered_data.to_html(escape=False) # to html\n",
    "\n",
    "    return display(HTML(filtered_data))\n",
    "    # return filtered_data[['verbal_report', reason]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y2Vz_ETwewnT",
   "metadata": {
    "id": "Y2Vz_ETwewnT"
   },
   "outputs": [],
   "source": [
    "# Show verbal reports for which the Expected value reason was assessed not to be used---i.e., confidence in usuing th reason was low, between 0 and 20\n",
    "show_verbal_reports_in_range(problems_reports, analyzed_reason, 0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-2rC48bLeeph",
   "metadata": {
    "id": "-2rC48bLeeph"
   },
   "outputs": [],
   "source": [
    "# Show verbal reports for which the Expected value reason was assessed to be used with high confidecne, i.e., between 80 to 100\n",
    "show_verbal_reports_in_range(problems_reports, analyzed_reason, 80, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vh2srIRoljtj",
   "metadata": {
    "id": "Vh2srIRoljtj"
   },
   "outputs": [],
   "source": [
    "# # Show all results\n",
    "# show_verbal_reports_in_range(problems_reports, analyzed_reason, 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sdlpW7AlmLLQ",
   "metadata": {
    "id": "sdlpW7AlmLLQ"
   },
   "source": [
    "# SURE OUTCOME\n",
    "Run the analyses for the sure outcome decision reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YTOuqci6nAbw",
   "metadata": {
    "id": "YTOuqci6nAbw"
   },
   "source": [
    "## Sure Outcome Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mVnjA_V8mZrS",
   "metadata": {
    "id": "mVnjA_V8mZrS"
   },
   "outputs": [],
   "source": [
    "# Select the description for expected value\n",
    "analyzed_reason = \"sure outcome\"\n",
    "selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == analyzed_reason, 'decision reason description'].values[0]\n",
    "\n",
    "# Create a list for storing prompts for the expected value reason\n",
    "sure_outcome_prompts = []\n",
    "\n",
    "# Generate prompts for the specific decision reason\n",
    "for _, row in problems_reports.iterrows():\n",
    "\n",
    "    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n",
    "    prompt = generate_prompt(\n",
    "        prompt1,\n",
    "        row['decision_problem'],\n",
    "        selected_description,  # Use the selected description\n",
    "        row['verbal_report']\n",
    "    )\n",
    "    sure_outcome_prompts.append(prompt)\n",
    "\n",
    "print(sure_outcome_prompts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sy28K58XnD3M",
   "metadata": {
    "id": "sy28K58XnD3M"
   },
   "source": [
    "## Sure Outcome LLAMA evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FLk9D_u-nIZB",
   "metadata": {
    "id": "FLk9D_u-nIZB"
   },
   "outputs": [],
   "source": [
    "# list for storing the output from the LLAMA model\n",
    "sure_outcome_eval = []\n",
    "\n",
    "# new column in the problems_reports data set for stroting the confidence assesments\n",
    "# remind that selected reason was set to 'expected value'\n",
    "problems_reports[analyzed_reason] = None\n",
    "\n",
    "# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n",
    "for i, prompt in enumerate(sure_outcome_prompts):\n",
    "\n",
    "    # response from LLAMA\n",
    "    llama_response = LLAMA.text_generation(prompt, max_new_tokens = 4000)\n",
    "    sure_outcome_eval.append(llama_response) # save the response to the sure_outcome_eval list\n",
    "\n",
    "    # extract the confidence value from the response\n",
    "    confidence_assesment = extract_confidence(llama_response)\n",
    "\n",
    "    # confidence value into the data\n",
    "    problems_reports.at[i, analyzed_reason] = confidence_assesment\n",
    "\n",
    "    # monitor progress\n",
    "    print(str(i) + '/' + str(problems_reports.shape[0]-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fGK3fCVSnuPD",
   "metadata": {
    "id": "fGK3fCVSnuPD"
   },
   "outputs": [],
   "source": [
    "# Show verbal reports for which the Sure outcome reason was assessed not to be used---i.e., confidence in usuing th reason was low, between 0 and 20\n",
    "show_verbal_reports_in_range(problems_reports, 'sure outcome', 0, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4mDf7-ZZn7Bs",
   "metadata": {
    "id": "4mDf7-ZZn7Bs"
   },
   "outputs": [],
   "source": [
    "# Show verbal reports for which the Sure outcome reason was assessed to be used with high confidecne, i.e., between 80 to 100\n",
    "show_verbal_reports_in_range(problems_reports, 'sure outcome', 80, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NEuJ93I2tZDp",
   "metadata": {
    "id": "NEuJ93I2tZDp"
   },
   "source": [
    "# Clean up the notebook (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9VXklc_-tdg1",
   "metadata": {
    "id": "9VXklc_-tdg1"
   },
   "outputs": [],
   "source": [
    "# Clear all variables\n",
    "%reset -f\n",
    "\n",
    "# Clear all outputs\n",
    "from IPython.display import clear_output\n",
    "clear_output()\n",
    "\n",
    "# Restart runtime\n",
    "import os\n",
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
