{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8df0bbd",
   "metadata": {},
   "source": [
    "# Identifying decision reasons (part 1)\n",
    "In this exercise, we will explore the capabilities of LLMs to identify decision reasons in verbal reports using the Hugging Face (HF) ecosystem. \n",
    "\n",
    "By the end of this exercise, you will have learned how to:\n",
    "- Design a zero-shot prompt\n",
    "- Have large models on the Hugging Face servers evaluate your prompts  \n",
    "- Validate the model output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979416b3c30fb86b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Using Notebook Environments \n",
    "1. To run a cell, press `shift + enter`. The notebook will execute the code in the cell and move to the next cell. If the cell contains a markdown cell (text only), it will render the markdown and move to the next cell.\n",
    "2. Since cells can be executed in any order and variables can be over-written, you may at some point feel that you have lost track of the state of your notebook. If this is the case, you can always restart the kernel by clicking Runtime in the menu bar (if you're using Colab) and selecting `Restart runtime`. This will clear all variables and outputs.\n",
    "3. The final variable in a cell will be printed on the screen. If you want to print multiple variables, use the `print()` function as usual.\n",
    "\n",
    "Notebook environments support code cells and markdown (text) cells. For the purposes of this workshop, markdown cells are used to provide high-level explanations of the code. More specific details are provided in the code cells themselves in the form of comments (lines beginning with `#`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c616368742ccde73",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:  # If in Google Colab environment\n",
    "    \n",
    "    # Installing requisite packages\n",
    "    !pip install huggingface_hub &> /dev/null\n",
    "\n",
    "    # Change working directory to day_1\n",
    "    %cd /content/drive/MyDrive/llms_egproc/exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c24c2ed829f80",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We begin by loading the requisite packages. For those coming from R, packages in Python are sometimes given shorter names for use in the code via the `import <name> as <nickname>` syntax (e.g. `import pandas as pd`). These are usually standardized nicknames. We here make use three packages:\n",
    "\n",
    "1. `pandas`: A very popular package for reading and manipulating data in python.\n",
    "2. `huggingface_hub`: A package for extracting features from text data using transformer-based models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267fad62d6f82855",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf097acf12ea11",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Hello world\n",
    "\n",
    "- setup the inference client\n",
    "- try out some stupid prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca015d4695f403",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Define sentences\n",
    "sentences = [\n",
    "    \"I feel great this morning\",\n",
    "    \"I am feeling very good today\",\n",
    "    \"I am feeling terrible\"\n",
    "]\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Extract features\n",
    "features = model.encode(sentences)\n",
    "\n",
    "# Print the features as a pandas dataframe\n",
    "pd.DataFrame(features, index=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80664214bd695bf0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "**TASK 1**: Have a scroll through the features printed by the cell. Can you see that the features of the first two sentences are more similar to each other (i.e., have similar numerical values) than they are to the third sentence? Why do you think this is the case?\n",
    "\n",
    "**TASK 2**: Try to add another sentence to the `sentences` list defined above. Use one of the existing sentences but replace one or two words with a synonym. For instance, you could change \"I feel *great* this morning\" to \"I feel *fantastic* this morning\". Then rerun the cell. What do you notice about the features of this new sentence compared to the original?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc8d5804ede6511",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Loading the prompt template (\n",
    "\n",
    "- load the prompt template\n",
    "- understand its components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb44bcf-bb8a-4603-8c25-33b1537399e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in prompt\n",
    "prompt = pd.read_csv(...)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4830e24a5d45430d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Constructing the prompt \n",
    "\n",
    "- read in the decision problems and reports\n",
    "- combine them to vector of complete prompts\n",
    "- look at 1 complete prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81690eb2b6590715",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Give it a test\n",
    "\n",
    "- run first prompt\n",
    "- make sense of output relative to prompt\n",
    "- maybe try again or next prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111710b7-d8cc-4214-b773-a76b1aac9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run the whole thing\n",
    "\n",
    "- run all responses for reason 1 \n",
    "- look at distribution of reason 1 confidences\n",
    "- evaluate verbal reports with high versus low confidence\n",
    "- repeat for other reasons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2031b74f-b268-42a8-ba93-2a9b6c6fbb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
