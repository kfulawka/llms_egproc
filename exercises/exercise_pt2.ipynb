{"cells":[{"cell_type":"markdown","id":"efc8d5804ede6511","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"efc8d5804ede6511"},"source":["# Part 2: Choose your adventure\n","In this part you can explore differnt ways of improving the overall performance of the identification of decision reasons:\n","\n","- A. Improve the prompt --- in this section you can edit our prompt, or even writ your own from scratch\n","- B. Other reasons --- in this section you can test your own decision reasons\n","- C. Better model --- in this section you can test models other than LLAMA and compare the performance between them\n"]},{"cell_type":"markdown","id":"c616368742ccde73","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"c616368742ccde73"},"source":["# Environment Setup\n","The code in this section mounts the google drive, loads packages, data, and functions from the previous part. You can simply clikc on the arrow to run all cells withouth unfolding this section.\n","\n","1. You must trun the code in this section to run the analyses in the following sectinos.\n","2. The 'adventures' are however indepnedent of each other\n","3. Don't forget to set the access token!\n"]},{"cell_type":"markdown","source":["## Monut google drive"],"metadata":{"id":"Ncfzwq_D1ElT"},"id":"Ncfzwq_D1ElT"},{"cell_type":"code","execution_count":null,"id":"initial_id","metadata":{"jupyter":{"outputs_hidden":true},"id":"initial_id","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722262360461,"user_tz":-120,"elapsed":19588,"user":{"displayName":"Kamil FuÅ‚awka","userId":"16000584420062196934"}},"outputId":"0fe03e2c-8097-4ef1-cdaf-694e678c9de7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# mount googl drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","source":["## Import the packacges and set the working directory"],"metadata":{"id":"1k98J5oh1HRh"},"id":"1k98J5oh1HRh"},{"cell_type":"code","source":["import sys\n","import os\n","import re\n","import textwrap\n","from IPython.display import display, HTML\n","import pandas as pd\n","from huggingface_hub import InferenceClient\n","\n","# the code below installs huggingface hub if it's missing\n","if 'google.colab' in sys.modules:  # If in Google Colab environment\n","\n","    # Installing requisite packages\n","    !pip install huggingface_hub &> /dev/null\n","\n","# this sets the working directory to the exercises folder\n","base_path = '/content/drive/My Drive/llms_egproc/exercises/'\n","os.chdir(base_path)"],"metadata":{"id":"pEeXB1mE0zaF"},"id":"pEeXB1mE0zaF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Read in data"],"metadata":{"id":"2ZeNJuWk6KKr"},"id":"2ZeNJuWk6KKr"},{"cell_type":"code","source":["# read in decision problems, decision reasons, and verbal reports\n","decision_problems = pd.read_csv('data/decision_problems.csv', encoding = 'utf-8')\n","decision_reasons = pd.read_csv('data/decision_reasons.csv', encoding = 'utf-8')\n","verbal_reports = pd.read_csv('data/verbal_reports.csv', encoding = 'utf-8')\n","\n","# merge verbal reports with decision problems\n","problems_reports = pd.merge(decision_problems, verbal_reports, on = 'problem_id')\n","\n","# the prompt\n","prompt_path = 'prompts/prompt_v1.txt'\n","\n","# Open the file and read its contents\n","with open(prompt_path, 'r') as file:\n","    prompt_base= file.read()"],"metadata":{"id":"KqENHCDv6MQm"},"id":"KqENHCDv6MQm","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the functions from ex1"],"metadata":{"id":"jjoaqwCSDfBa"},"id":"jjoaqwCSDfBa"},{"cell_type":"code","source":["# function for constructing the full prompt\n","def generate_prompt(prompt, decision_problem, decision_reason, verbal_report):\n","    \"\"\"\n","    Replaces placeholders in the prompt with the given decision problem, decision reason, and verbal report.\n","    \"\"\"\n","    # Replace placeholders with actual values\n","    filled_prompt = prompt.replace(\"DECISION_PROBLEM\", decision_problem)\n","    filled_prompt = filled_prompt.replace(\"DECISION_REASON\", decision_reason)\n","    filled_prompt = filled_prompt.replace(\"VERBAL_REPORT\", verbal_report)\n","\n","    return filled_prompt\n","\n","# Function for extracting confidence assessments\n","def extract_confidence(s):\n","    \"\"\"\n","    Extracts an integer value from a string enclosed between @ or @@ symbols.\n","    \"\"\"\n","    # Regular expression to match patterns like @number@ or @@number@@\n","    pattern = r'@+(\\s*\\d+\\s*)@+'\n","\n","    # Search for the pattern in the string\n","    match = re.search(pattern, s)\n","\n","    if match:\n","        # Extract the number and convert it to an integer\n","        number_str = match.group(1).strip()\n","        return int(number_str)\n","\n","    return None\n","\n","# Function to wrap text\n","def wrap_text(text, width=100):\n","    return \"<br>\".join(textwrap.wrap(text, width))\n","\n","# display data frames in HTML\n","def disp_tab(dd):\n","    dd = dd.to_html(escape=False)\n","    return display(HTML(dd))\n","\n","# Function to show verbal reports with assigned numbers in a specified range\n","def show_verbal_reports_in_range(data, reason, min_confidence, max_confidence):\n","    \"\"\"\n","    Shows verbal reports for which the model assigned a confidence within the specified range.\n","    \"\"\"\n","    filtered_data = data[(data[reason] >= min_confidence) & (data[reason] <= max_confidence)] # filter by the specified range\n","\n","     # wrap the text for nicer display\n","    filtered_data.loc[:, 'verbal_report'] = filtered_data['verbal_report'].apply(wrap_text)\n","    filtered_data.loc[:, 'decision_problem'] = filtered_data['decision_problem'].apply(lambda x: wrap_text(x, width=40))\n","\n","    # select only the columns with report and confidence assesment\n","    filtered_data = filtered_data[['decision_problem', 'verbal_report', 'choice', reason]]\n","\n","    return disp_tab(filtered_data)\n","    # return filtered_data[['verbal_report', reason]]"],"metadata":{"id":"JbJQyYkpDvPd"},"id":"JbJQyYkpDvPd","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Set the access token\n","\n"],"metadata":{"id":"Rd8S1JlvBSSt"},"id":"Rd8S1JlvBSSt"},{"cell_type":"code","source":["API_TOKEN = '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'"],"metadata":{"id":"jLvKZW5BBVbw"},"id":"jLvKZW5BBVbw","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"81690eb2b6590715","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"81690eb2b6590715"},"source":["# Adeventure 1: Improve the Prompt\n","\n","- or make it worse (e.g., remove chain of thoughts)\n","- see how the model reacts to changes in the prompts\n"]},{"cell_type":"code","source":["# We should use the same model to compare the diiferences a prompt can make:\n","LLM_version = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n","LLM = InferenceClient(model = LLM_version, token = API_TOKEN)"],"metadata":{"id":"hC8lGoIDIbrQ"},"id":"hC8lGoIDIbrQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Also let's fix the decision reason"],"metadata":{"id":"6jXK9JXPJBVf"},"id":"6jXK9JXPJBVf"},{"cell_type":"code","source":["# Set up the prompts for a decision reason of your choice\n","# here are the avilable reasons\n","disp_tab(decision_reasons)"],"metadata":{"id":"2fE6-N5bJH9h"},"id":"2fE6-N5bJH9h","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Set the selected_reason variable to the rason of your choice"],"metadata":{"id":"Yucqh90f2F02"},"id":"Yucqh90f2F02"},{"cell_type":"code","source":["# set the decision reason\n","selected_reason = 'maximum outcome' # change to your reason of choice\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]"],"metadata":{"id":"3nIuj1TcJKSs"},"id":"3nIuj1TcJKSs","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## First get model responses for our prompt\n"],"metadata":{"id":"Kse5y9JdG9_p"},"id":"Kse5y9JdG9_p"},{"cell_type":"code","source":["# Create a list for storing prompts\n","filled_prompts_p1 = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base, # this is our prompt from ex1\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    filled_prompts_p1.append(prompt)"],"metadata":{"id":"l20flR4jI1KL"},"id":"l20flR4jI1KL","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Second, run the model with our prompt\n","While you wait for the results, you can star working on your own prompt in the next code snippet."],"metadata":{"id":"w3lZBDRTJh8j"},"id":"w3lZBDRTJh8j"},{"cell_type":"code","source":["# list for storing the output for prompt 1\n","LLM_P1_results = []\n","\n","# column name for storage of the confidence values from prompt 1\n","llm_P1_res_col = 'llm_P1_confidence_res'\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","problems_reports[llm_P1_res_col] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts_p1):\n","\n","    # response from LLAMA\n","    LLM_P1_response = LLM1.text_generation(prompt, max_new_tokens = 4000)\n","    LLM_P1_results.append(LLM_P1_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM1_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, llm_P1_res_col] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"id":"hGf4HjluIkpN"},"id":"hGf4HjluIkpN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Prepare your own prompt\n","#### Keep in mind!\n","1. The function `generate_prompt` requiers that the `base prompt` contains the place holders for the decision reason, decision problem and verbal report in the following forms: DECISION_PROBLEM, DECISION_REASON, VERBAL_REPORT.\n","2. The function `extract_confidence` assumes tha the model outputs confidence assesment between the @@ symbols, so it' best if you don't change the assesment type.\n","\n","The rest can be changed as you please."],"metadata":{"id":"oWOeCXdYJ6bh"},"id":"oWOeCXdYJ6bh"},{"cell_type":"code","source":["prompt_base_p2 = \"\"\"\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","You are a decision analyst who accurately identifies whether decision reasons are present or absent in verbal reports of people written after they made a choice between two monetary lotteries of a decision problem.\n","\n","Available information â€”\n","A decision problem poses a choice between two lotteries, A and B, offering different monetary outcomes with different probabilities.\n","\n","A decision reason specifies a rule to decide which of the two lotteries is preferred by the reason. The decision reason prefers A or B or is indifferent between the lotteries.\n","\n","A verbal report written by an individual describes, in retrospect, the individualâ€™s deliberation process used to choose one of the lotteries of the decision problem.\n","\n","\n","Task description â€”\n","Your task is to assess, based on the verbal report, whether the individual used the reason to make the decision.\n","The wording in the verbal report does not need to match the decision reason verbatim;\n","consider other wordings but make sure that the essence of the reason is clearly reflected by the verbal report.\n","Perform your analysis in three steps.\n","\n","Step 1: Asses if the decision reason can be applied to the decision problem.\n","Evaluate whether the information relevant to the decision reason can be derived from the lotteries' outcomes and probabilities and summarize this information.\n","Proceed to Step 2.\n","\n","Step 2: Assess the verbal report.\n","First, evaluate and summarize the outcome and probability information considered by the individual.\n","Second, evaluate and summarize the individualâ€™s justification for the choice.\n","Focus on the described deliberation process and ignore information about the individualâ€™s final choice.\n","Proceed to Step 3.\n","\n","Step 3: Assess confidence in the decision reasonâ€™s use.\n","First, compare the outcome and probability information relevant to the decision reason and those considered by the individual.\n","Second, compare the decision reasonâ€™s rule to the individualâ€™s justification for the choice.\n","Based on these two comparisons, return a value between 0 (certainly not used) and 100 (certainly used), reflecting your confidence that the individual used the decision reason to make the decision.\n","\n","\n","Output format â€”\n","Return the results of your assessment in the following format.\n","Return the confidence value by inserting it between two @@ symbols.\n","Only insert numbers between 0 (certainly not used) and 100 (certainly used).\n","\n","Here is a template for the output format:\n","Confidence: @ insert confidence value @\n","\n","<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","Consider the following decision problem, decision reason, and verbal report:\n","\n","\n","Decision problem ---\n","DECISION_PROBLEM\n","\n","\n","Decision reason ---\n","DECISION_REASON\n","\n","\n","Verbal report ---\n","VERBAL_REPORT\n","\n","\n","Task ---\n","Perform the confidence assessment step-by-step. Closely follow the steps previously outlined. Describe your reasoning before you arrive at an answer.\n","In the end, provide your confidence assessment that the decision reason was used by the individual using the specified output format.\n","<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\"\"\""],"metadata":{"id":"IO3DYbDVKOKR"},"id":"IO3DYbDVKOKR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate new prompts based on your prompt P2 for the previously set decision reason"],"metadata":{"id":"IPmxnTaxLG9t"},"id":"IPmxnTaxLG9t"},{"cell_type":"code","source":["# Create a list for storing prompts\n","filled_prompts_p2 = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base_p2, # here we are now passing your custom prompt\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    filled_prompts_p2.append(prompt)"],"metadata":{"id":"UeOc81o6LPHC"},"id":"UeOc81o6LPHC","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run the model"],"metadata":{"id":"EZuttxyKLXap"},"id":"EZuttxyKLXap"},{"cell_type":"code","source":["# list for storing the output for prompt 2\n","LLM_P2_results = []\n","\n","# column name for storage of the confidence values from prompt 1\n","llm_P2_res_col = 'llm_P2_confidence_res'\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","problems_reports[llm_P2_res_col] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts_p2):\n","\n","    # response from LLAMA\n","    LLM_P2_response = LLM1.text_generation(prompt, max_new_tokens = 4000)\n","    LLM_P2_results.append(LLM_P2_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM_P2_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, llm_P2_res_col] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"id":"5mCv5tgHLaSH"},"id":"5mCv5tgHLaSH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Compare results\n","First have a look at results generated with our prompt"],"metadata":{"id":"iT0QvJpuMZuN"},"id":"iT0QvJpuMZuN"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'llm_P1_confidence_res', 80, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"wAz52SGLMdal"},"id":"wAz52SGLMdal","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now have a look at the results generated with your prompt"],"metadata":{"id":"IAEqSu6SMpEp"},"id":"IAEqSu6SMpEp"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'llm_P2_confidence_res', 80, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"Xyrqz_WcMs2m"},"id":"Xyrqz_WcMs2m","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can also compare the full text of the model 'thinking' directly, for corresponding verbal reports:\n","\n","(change the index numer in both code snippets to compare resposned corresponding to the same verbal reports)"],"metadata":{"id":"kk-9QX2yM1Zq"},"id":"kk-9QX2yM1Zq"},{"cell_type":"code","source":["print(LLM_P1_results[1]) # first LLM response based on prompt P1 (ours)"],"metadata":{"id":"cpfVg47jM2ho"},"id":"cpfVg47jM2ho","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(LLM_P2_results[1]) # first LLM response based on prompt P1 (ours)"],"metadata":{"id":"tU9N9IfsNJjf"},"id":"tU9N9IfsNJjf","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1d00f880-e9b0-44f5-9328-a4fc1e39a815","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"1d00f880-e9b0-44f5-9328-a4fc1e39a815"},"source":["# Adventure B: other reasons\n","\n","In this adventure you can test your own reasons."]},{"cell_type":"code","source":["# Let's set the InferenceClient first.\n","LLM_version = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n","LLM = InferenceClient(model = LLM_version, token = API_TOKEN)"],"metadata":{"id":"ncKYB7rHN5MH"},"id":"ncKYB7rHN5MH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Have a look at our reasons for inspiration"],"metadata":{"id":"uAVGkuejN7k9"},"id":"uAVGkuejN7k9"},{"cell_type":"code","source":["# Set up the prompts for a decision reason of your choice\n","# here are the avilable reasons\n","disp_tab(decision_reasons)"],"metadata":{"id":"RpF0_pKGN_wE"},"id":"RpF0_pKGN_wE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now create your own reason by filling in the code in the next snippet"],"metadata":{"id":"kSRiMItUOTIF"},"id":"kSRiMItUOTIF"},{"cell_type":"code","source":["# set the decision reason\n","new_reason = 'regret' # change to the name of your reason --- this will be used as a column name in the problems_reasons data for storing the confidence assesments\n","\n","# now add a description --- this description will be used in the prompt!\n","new_reason_description = 'The reason considers the outcomes of each lottery. The sum of all pairwise differences of outcomes between the lotteries is considered important. The reason prefers the lottery with the more favorable sum of outcome differences.'"],"metadata":{"id":"wlJ-skarOYyM"},"id":"wlJ-skarOYyM","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generate the prompts with your reason"],"metadata":{"id":"U4vqFzU_OyLk"},"id":"U4vqFzU_OyLk"},{"cell_type":"code","source":["# Create a list for storing prompts\n","filled_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base, # here we are now passing your prompt\n","        row['decision_problem'],\n","        new_reason_description,  # Use the selected description that you provided above\n","        row['verbal_report']\n","    )\n","    filled_prompts.append(prompt)"],"metadata":{"id":"QqRpnJ9xO3nd"},"id":"QqRpnJ9xO3nd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(filled_prompts[1])"],"metadata":{"id":"bZhV0I8cPKC5"},"id":"bZhV0I8cPKC5","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run the model"],"metadata":{"id":"iZ1B_pk8Pl4U"},"id":"iZ1B_pk8Pl4U"},{"cell_type":"markdown","source":["#### Run the anlaysis for a random prompt"],"metadata":{"id":"sAfwcd_23rcW"},"id":"sAfwcd_23rcW"},{"cell_type":"code","source":["response = LLM.text_generation(filled_prompts[5], max_new_tokens = 4000)"],"metadata":{"id":"rUuRYRjM3vBk"},"id":"rUuRYRjM3vBk","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(response[0])"],"metadata":{"id":"DZxqdVt-36n-"},"id":"DZxqdVt-36n-","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Or iterate over the entire list of prompts (the entire data set)"],"metadata":{"id":"xy49sG6e38FE"},"id":"xy49sG6e38FE"},{"cell_type":"code","source":["# list for storing the output\n","LLM_results = []\n","\n","# column name for storage of the confidence values will be the name under `selected_reason`\n","problems_reports[new_reason] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts):\n","\n","    # response from LLAMA\n","    LLM_response = LLM.text_generation(prompt, max_new_tokens = 4000)\n","    LLM_results.append(LLM_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, new_reason] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"id":"_oUyoKlbPq6f"},"id":"_oUyoKlbPq6f","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check the results!"],"metadata":{"id":"f-atOaVQQY9x"},"id":"f-atOaVQQY9x"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, new_reason, 80, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"ANMGnmGZQbG6"},"id":"ANMGnmGZQbG6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(LLM_results[0]) # first LLM response based"],"metadata":{"id":"mGphSUGMQeb2"},"id":"mGphSUGMQeb2","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"4830e24a5d45430d","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"4830e24a5d45430d"},"source":["# Adeventure C: Better model"]},{"cell_type":"markdown","source":["## Select a decision reason for comparioson of models"],"metadata":{"id":"HElsynQ56i4x"},"id":"HElsynQ56i4x"},{"cell_type":"code","source":["# Set up the prompts for a decision reason of your choice\n","# here are the avilable reasons\n","disp_tab(decision_reasons)"],"metadata":{"id":"r6l6bz199fwe"},"id":"r6l6bz199fwe","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate prompts with the selected reason"],"metadata":{"id":"rxLOVwJcARFB"},"id":"rxLOVwJcARFB"},{"cell_type":"code","source":["selected_reason = 'maximum outcome' # change to your reason of choice\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]\n","\n","# Create a list for storing prompts for the expected value reason\n","filled_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base,\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    filled_prompts.append(prompt)"],"metadata":{"id":"Qkc890nt9Q_a"},"id":"Qkc890nt9Q_a","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(filled_prompts[0])"],"metadata":{"id":"W29O0-SZCyzJ"},"id":"W29O0-SZCyzJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run the analysis with model from excercise 1"],"metadata":{"id":"ekPBcBOMAgt1"},"id":"ekPBcBOMAgt1"},{"cell_type":"code","source":["LLM1_version = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n","LLM1 = InferenceClient(model = LLM1_version, token = API_TOKEN)\n","\n","# list for storing the output from the model 1\n","LLM1_results = []\n","\n","# column name for storage of the confidence values\n","llm1_res_col = 'llm1_confidence_res'\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","problems_reports[llm1_res_col] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts):\n","\n","    # response from LLAMA\n","    LLM1_response = LLM1.text_generation(prompt, max_new_tokens = 4000)\n","    LLM1_results.append(LLM1_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM1_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, llm1_res_col] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"id":"PO-UB_jY6DNc"},"id":"PO-UB_jY6DNc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run the analysis with another model\n","We propose to use the recently upgraded version of the model used in ex1 and above.\n","\n","The list of avilable LLAMA-3 models (from worst to best):\n","* `meta-llama/Meta-Llama-3-8B-Instruct`\n","* `meta-llama/Meta-Llama-3.1-8B-Instruct`\n","* `meta-llama/Meta-Llama-3-70B-Instruct`\n","* `meta-llama/Meta-Llama-3.1-70B-Instruct`\n","* `meta-llama/Meta-Llama-3.1-405B-Instruct`\n","\n","The last one is the most recent and best LLAMA model, and it's also the largest one. It will be much slower to run analyses with it. So instead of looping throught the entire dataset, consider running the same **one** prompt with this and other models."],"metadata":{"id":"oyRSTcVoB8Js"},"id":"oyRSTcVoB8Js"},{"cell_type":"code","source":["LLM2_version = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"  # most recent updated LLAMA family\n","LLM2 = InferenceClient(model = LLM2_version, token = API_TOKEN)\n","\n","# list for storing the output from the model 1\n","LLM2_results = []\n","\n","# column name for storage of the confidence values\n","llm2_res_col = 'llm2_confidence_res'\n","\n","# new column in the problems_reports data set for storing the confidence assesments\n","problems_reports[llm2_res_col] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts):\n","\n","    # response from LLAMA\n","    LLM2_response = LLM2.text_generation(prompt, max_new_tokens = 4000)\n","    LLM2_results.append(LLM2_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM2_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, llm2_res_col] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"id":"tVekGHAqCC9V"},"id":"tVekGHAqCC9V","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Compare the results from both models\n","First, high confidence reports from model 1"],"metadata":{"id":"fwxsBs4IE03h"},"id":"fwxsBs4IE03h"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'llm1_confidence_res', 80, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"hj4EbWNAE_mk"},"id":"hj4EbWNAE_mk","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Second, high confidence reports from model 2"],"metadata":{"id":"2-WGxYppFWXH"},"id":"2-WGxYppFWXH"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'llm2_confidence_res', 80, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"h-C7MpaRFafu"},"id":"h-C7MpaRFafu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can also compare the full text of the model 'thinking' directly, for corresponding verbal reports:\n","\n","(change the index numer in both code snippets to compare resposned corresponding to the same verbal reports)"],"metadata":{"id":"yqDWqG_IF5Do"},"id":"yqDWqG_IF5Do"},{"cell_type":"code","source":["print(LLM1_results[1]) # response to verbal report 1 from model 1"],"metadata":{"id":"i5yXlh4wF-pR"},"id":"i5yXlh4wF-pR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Other ways of comparing the models\n","Think of other ways of comparing the model performance and explore them. Feel free to ask us questions on how to set up what you have in mind."],"metadata":{"id":"B6tMod4fFpPj"},"id":"B6tMod4fFpPj"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[],"collapsed_sections":["c616368742ccde73","4830e24a5d45430d"]}},"nbformat":4,"nbformat_minor":5}