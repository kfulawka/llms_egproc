{"cells":[{"cell_type":"markdown","id":"efc8d5804ede6511","metadata":{"collapsed":false,"id":"efc8d5804ede6511","jupyter":{"outputs_hidden":false}},"source":["# Part 2: Choose your adventure\n","In this part, you can explore different ways of improving the the reason identification pipeline:\n","\n","- A. Improve the prompt --- in this section you can edit our prompt, or even writ your own from scratch\n","- B. Other reasons --- in this section you can test your own decision reasons\n","- C. Better model --- in this section you can test models other than LLAMA and compare the performance between them\n"]},{"cell_type":"markdown","id":"c616368742ccde73","metadata":{"collapsed":false,"id":"c616368742ccde73","jupyter":{"outputs_hidden":false}},"source":["### Environment Setup\n","The code in this section mounts the google drive, loads packages, data, and functions from the previous part.\n","\n","You can simply click on the arrow to run all cells withouth unfolding this section.\n","\n","1. You **must** run the code in this section to run the analyses in the following sectinos.\n","2. The 'adventures' are however indepnedent of each other\n","3. Don't forget to set the access token!\n"]},{"cell_type":"code","execution_count":null,"id":"f6cb834a-bfe8-4aa7-bd38-8c31af69512e","metadata":{"id":"f6cb834a-bfe8-4aa7-bd38-8c31af69512e"},"outputs":[],"source":["# mount google drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"id":"pEeXB1mE0zaF","metadata":{"id":"pEeXB1mE0zaF"},"outputs":[],"source":["import sys\n","import os\n","import re\n","import textwrap\n","from IPython.display import display, HTML\n","import pandas as pd\n","from huggingface_hub import InferenceClient\n","\n","# the code below installs huggingface hub if it's missing\n","if 'google.colab' in sys.modules:  # If in Google Colab environment\n","\n","    # Installing requisite packages\n","    !pip install huggingface_hub &> /dev/null\n","\n","# this sets the working directory to the exercises folder\n","os.chdir('/content/drive/My Drive/llms_egproc/exercises/')"]},{"cell_type":"markdown","id":"2ZeNJuWk6KKr","metadata":{"id":"2ZeNJuWk6KKr"},"source":["### Read the data"]},{"cell_type":"code","execution_count":null,"id":"KqENHCDv6MQm","metadata":{"id":"KqENHCDv6MQm"},"outputs":[],"source":["# read in decision problems, decision reasons, and verbal reports\n","decision_problems = pd.read_csv('data/decision_problems.csv', encoding = 'utf-8')\n","decision_reasons = pd.read_csv('data/decision_reasons.csv', encoding = 'utf-8')\n","verbal_reports = pd.read_csv('data/verbal_reports.csv', encoding = 'utf-8')\n","\n","# merge verbal reports with decision problems\n","problems_reports = pd.merge(decision_problems, verbal_reports, on = 'problem_id')\n","\n","# the prompt\n","prompt_path = 'prompts/prompt_v1.txt'\n","\n","# Open the file and read its contents\n","with open(prompt_path, 'r') as file:\n","    prompt_base= file.read()"]},{"cell_type":"markdown","id":"jjoaqwCSDfBa","metadata":{"id":"jjoaqwCSDfBa"},"source":["### Load the functions from exercise 1"]},{"cell_type":"code","execution_count":null,"id":"JbJQyYkpDvPd","metadata":{"id":"JbJQyYkpDvPd"},"outputs":[],"source":["# function for constructing the full prompt\n","def generate_prompt(prompt, decision_problem, decision_reason, verbal_report):\n","\n","    # Replace placeholders with actual values\n","    filled_prompt = prompt.replace(\"DECISION_PROBLEM\", decision_problem)\n","    filled_prompt = filled_prompt.replace(\"DECISION_REASON\", decision_reason)\n","    filled_prompt = filled_prompt.replace(\"VERBAL_REPORT\", verbal_report)\n","\n","    return filled_prompt\n","\n","# Function for extracting confidence assessments\n","def extract_confidence(s):\n","\n","    # Regular expression to match patterns like @number@ or @@number@@\n","    pattern = r'@+(\\s*\\d+\\s*)@+'\n","\n","    # Search for the pattern in the string\n","    match = re.search(pattern, s)\n","\n","    if match:\n","        # Extract the number and convert it to an integer\n","        number_str = match.group(1).strip()\n","        return int(number_str)\n","\n","    return None\n","\n","# Function to wrap text\n","def wrap_text(text, width=100):\n","    return \"<br>\".join(textwrap.wrap(text, width))\n","\n","# display data frames in HTML\n","def disp_tab(dd):\n","    dd = dd.map(lambda x: wrap_text(str(x), width=40))\n","    dd = dd.to_html(escape=False)\n","    return display(HTML(dd))\n","\n","# Function to show verbal reports with confidence assesment below and above specified values\n","def show_verbal_reports_in_range(data, reason, min_threshold = 100, max_treshold = 0, show_na = False):\n","\n","    # if true, display the responses for which conf assessemnt couldn't be extracted\n","    if show_na:\n","        filtered_data = data[data[reason] == 999]\n","    else:\n","        filtered_data = data[(data[reason] <= min_threshold) | (data[reason] >= max_treshold) & (data[reason] != 999)]\n","\n","     # wrap the text for nicer display\n","    filtered_data.loc[:, 'verbal_report'] = filtered_data['verbal_report'].apply(wrap_text)\n","    filtered_data.loc[:, 'decision_problem'] = filtered_data['decision_problem'].apply(lambda x: wrap_text(x, width=40))\n","\n","    # select only the columns with report and confidence assesment\n","    filtered_data = filtered_data[['decision_problem', 'verbal_report', 'choice', reason]]\n","    filtered_data = filtered_data.to_html(escape=False) # to html\n","\n","    return display(HTML(filtered_data))"]},{"cell_type":"markdown","id":"Rd8S1JlvBSSt","metadata":{"id":"Rd8S1JlvBSSt"},"source":["### Set the access token\n","Again, use the one you received via email."]},{"cell_type":"code","execution_count":null,"id":"jLvKZW5BBVbw","metadata":{"id":"jLvKZW5BBVbw"},"outputs":[],"source":["API_TOKEN = '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'"]},{"cell_type":"markdown","id":"81690eb2b6590715","metadata":{"collapsed":false,"id":"81690eb2b6590715","jupyter":{"outputs_hidden":false}},"source":["# Adeventure A: Improve the Prompt\n","\n","In this adventure, you get to play around with the prompt. Make it better or worse. For instance, try removing the chain of thought component. See how the model reacts."]},{"cell_type":"markdown","id":"fc52df58-6563-4044-a414-e5149651b59e","metadata":{"id":"fc52df58-6563-4044-a414-e5149651b59e"},"source":["### Setting up the inference client\n"]},{"cell_type":"code","execution_count":null,"id":"hC8lGoIDIbrQ","metadata":{"id":"hC8lGoIDIbrQ"},"outputs":[],"source":["# Setting up Llama 3.1 8B\n","LLM_version = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","LLM = InferenceClient(model = LLM_version, token = API_TOKEN)"]},{"cell_type":"markdown","id":"6jXK9JXPJBVf","metadata":{"id":"6jXK9JXPJBVf"},"source":["### Fixing the decision reason\n","Keep `'maximum outcome'` or replace it with your own choice from the reason table."]},{"cell_type":"code","execution_count":null,"id":"2fE6-N5bJH9h","metadata":{"id":"2fE6-N5bJH9h"},"outputs":[],"source":["# here are the available reasons\n","disp_tab(decision_reasons)\n","\n","# set the decision reason\n","selected_reason = 'maximum outcome' # change to your reason of choice\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]"]},{"cell_type":"markdown","id":"oWOeCXdYJ6bh","metadata":{"id":"oWOeCXdYJ6bh"},"source":["### Adjust the prompt\n","\n","**Keep in mind!**\n","1. The function `generate_prompt` requires that the `prompt_base` contain the placeholders for the decision reason, decision problem, and verbal report in the following forms: DECISION_PROBLEM, DECISION_REASON, VERBAL_REPORT.\n","2. The function `extract_confidence` assumes that the model outputs a confidence assessment between the @@ symbols, so it's best to keep the assessment type the same.\n","\n","The rest can be changed as you please."]},{"cell_type":"code","execution_count":null,"id":"IO3DYbDVKOKR","metadata":{"id":"IO3DYbDVKOKR"},"outputs":[],"source":["prompt_base = \"\"\"\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","You are a decision analyst who accurately identifies whether decision reasons are present or absent in verbal reports of people written after they made a choice between two monetary lotteries of a decision problem.\n","\n","Available information —\n","A decision problem poses a choice between two lotteries, A and B, offering different monetary outcomes with different probabilities.\n","\n","A decision reason specifies a rule to decide which of the two lotteries is preferred by the reason. The decision reason prefers A or B or is indifferent between the lotteries.\n","\n","A verbal report written by an individual describes, in retrospect, the individual’s deliberation process used to choose one of the lotteries of the decision problem.\n","\n","\n","Task description —\n","Your task is to assess, based on the verbal report, whether the individual used the reason to make the decision.\n","The wording in the verbal report does not need to match the decision reason verbatim;\n","consider other wordings but make sure that the essence of the reason is clearly reflected by the verbal report.\n","Perform your analysis in three steps.\n","\n","Step 1: Asses if the decision reason can be applied to the decision problem.\n","Evaluate whether the information relevant to the decision reason can be derived from the lotteries' outcomes and probabilities and summarize this information.\n","Proceed to Step 2.\n","\n","Step 2: Assess the verbal report.\n","First, evaluate and summarize the outcome and probability information considered by the individual.\n","Second, evaluate and summarize the individual’s justification for the choice.\n","Focus on the described deliberation process and ignore information about the individual’s final choice.\n","Proceed to Step 3.\n","\n","Step 3: Assess confidence in the decision reason’s use.\n","First, compare the outcome and probability information relevant to the decision reason and those considered by the individual.\n","Second, compare the decision reason’s rule to the individual’s justification for the choice.\n","Based on these two comparisons, return a value between 0 (certainly not used) and 100 (certainly used), reflecting your confidence that the individual used the decision reason to make the decision.\n","\n","\n","Output format —\n","Return the results of your assessment in the following format.\n","Return the confidence value by inserting it between two @@ symbols.\n","Only insert numbers between 0 (certainly not used) and 100 (certainly used).\n","\n","Here is a template for the output format:\n","Confidence: @ insert confidence value @\n","\n","<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","Consider the following decision problem, decision reason, and verbal report:\n","\n","\n","Decision problem ---\n","DECISION_PROBLEM\n","\n","\n","Decision reason ---\n","DECISION_REASON\n","\n","\n","Verbal report ---\n","VERBAL_REPORT\n","\n","\n","Task ---\n","Perform the confidence assessment step-by-step. Closely follow the steps previously outlined. Describe your reasoning before you arrive at an answer.\n","In the end, provide your confidence assessment that the decision reason was used by the individual using the specified output format.\n","<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\"\"\""]},{"cell_type":"markdown","id":"IPmxnTaxLG9t","metadata":{"id":"IPmxnTaxLG9t"},"source":["### Run the prompts through the model"]},{"cell_type":"code","execution_count":null,"id":"UeOc81o6LPHC","metadata":{"id":"UeOc81o6LPHC"},"outputs":[],"source":["# Create a list for storing prompts\n","filled_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the selected reason\n","    prompt = generate_prompt(\n","        prompt_base, # here we are now passing your custom prompt\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    filled_prompts.append(prompt)\n","\n","# list for storing the output\n","LLM_results = []\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","# this will be the name of the selected reason\n","problems_reports[selected_reason] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, extract numerical estimates, and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts):\n","\n","    # response from LLAMA\n","    LLM_response = LLM.text_generation(prompt, max_new_tokens = 4000)\n","    LLM_results.append(LLM_response) # save the response\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM_response)\n","\n","    # store confidence value in the data\n","    problems_reports.at[i, selected_reason] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"]},{"cell_type":"markdown","id":"iT0QvJpuMZuN","metadata":{"id":"iT0QvJpuMZuN"},"source":["### Evaluate results\n","Inspect the outputs the model generated under prompt"]},{"cell_type":"code","execution_count":null,"id":"wAz52SGLMdal","metadata":{"id":"wAz52SGLMdal"},"outputs":[],"source":["# Show verbal reports with low and high confidence\n","show_verbal_reports_in_range(problems_reports, selected_reason, 20, 80) # remove 20, 80 to show all output"]},{"cell_type":"code","execution_count":null,"id":"cf6b1966-9c02-4b43-b0bd-fde84ca5809f","metadata":{"id":"cf6b1966-9c02-4b43-b0bd-fde84ca5809f"},"outputs":[],"source":["# show the complete LLM output for index of your choice (you can change 1 to any other valid index)\n","print(LLM_results[1])"]},{"cell_type":"markdown","id":"1d00f880-e9b0-44f5-9328-a4fc1e39a815","metadata":{"collapsed":false,"id":"1d00f880-e9b0-44f5-9328-a4fc1e39a815","jupyter":{"outputs_hidden":false}},"source":["# Adventure B: other reasons\n","\n","In this adventure, you can test your own reasons."]},{"cell_type":"markdown","id":"0f6b8c88-635d-45c5-a2d0-ce7ff9a24c8e","metadata":{"id":"0f6b8c88-635d-45c5-a2d0-ce7ff9a24c8e"},"source":["### Set up the inference client"]},{"cell_type":"code","execution_count":null,"id":"ncKYB7rHN5MH","metadata":{"id":"ncKYB7rHN5MH"},"outputs":[],"source":["# Let's set the InferenceClient first.\n","LLM_version = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n","LLM = InferenceClient(model = LLM_version, token = API_TOKEN)"]},{"cell_type":"markdown","id":"uAVGkuejN7k9","metadata":{"id":"uAVGkuejN7k9"},"source":["Have a look at our reasons for inspiration"]},{"cell_type":"code","execution_count":null,"id":"RpF0_pKGN_wE","metadata":{"id":"RpF0_pKGN_wE"},"outputs":[],"source":["# Set up the prompts for a decision reason of your choice\n","# here are the avilable reasons\n","disp_tab(decision_reasons)"]},{"cell_type":"markdown","id":"kSRiMItUOTIF","metadata":{"id":"kSRiMItUOTIF"},"source":["Now create your own reason by filling in the code in the next snippet"]},{"cell_type":"code","execution_count":null,"id":"wlJ-skarOYyM","metadata":{"id":"wlJ-skarOYyM"},"outputs":[],"source":["# set the decision reason\n","new_reason = 'regret' # change to the name of your reason --- this will be used as a column name in the problems_reasons data for storing the confidence assesments\n","\n","# now add a description --- this description will be used in the prompt!\n","new_reason_description = 'The reason considers the outcomes of each lottery. The sum of all pairwise differences of outcomes between the lotteries is considered important. The reason prefers the lottery with the more favorable sum of outcome differences.'"]},{"cell_type":"markdown","id":"U4vqFzU_OyLk","metadata":{"id":"U4vqFzU_OyLk"},"source":["### Run the reason through the model"]},{"cell_type":"code","execution_count":null,"id":"QqRpnJ9xO3nd","metadata":{"id":"QqRpnJ9xO3nd"},"outputs":[],"source":["# Create a list for storing prompts\n","filled_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base, # here we are now passing your prompt\n","        row['decision_problem'],\n","        new_reason_description,  # Use the new reason description that you provided above\n","        row['verbal_report']\n","    )\n","    filled_prompts.append(prompt)\n","\n","# list for storing the output\n","LLM_results = []\n","\n","# column name for storage of the confidence values will be the name under `new_reason`\n","problems_reports[new_reason] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts):\n","\n","    # response from LLAMA\n","    LLM_response = LLM.text_generation(prompt, max_new_tokens = 4000)\n","    LLM_results.append(LLM_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, new_reason] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"]},{"cell_type":"markdown","id":"f-atOaVQQY9x","metadata":{"id":"f-atOaVQQY9x"},"source":["### Evaluate results"]},{"cell_type":"code","execution_count":null,"id":"ANMGnmGZQbG6","metadata":{"id":"ANMGnmGZQbG6"},"outputs":[],"source":["# Show verbal reports with low and high confidence\n","show_verbal_reports_in_range(problems_reports, new_reason, 20, 80) # remove 20, 80 to show all output"]},{"cell_type":"code","execution_count":null,"id":"mGphSUGMQeb2","metadata":{"id":"mGphSUGMQeb2"},"outputs":[],"source":["# Show complete LLM output for index 0\n","print(LLM_results[2]) # first LLM response based"]},{"cell_type":"markdown","id":"4830e24a5d45430d","metadata":{"collapsed":false,"id":"4830e24a5d45430d","jupyter":{"outputs_hidden":false}},"source":["# Adeventure C: Better model\n","\n","In this adventure, you get to play around with other models."]},{"cell_type":"markdown","id":"58ede81f-d794-4fde-af3c-2ddf104255c3","metadata":{"id":"58ede81f-d794-4fde-af3c-2ddf104255c3"},"source":["### Set up an inference client with a new model\n","\n","Try out some of the larger Llama models with either 70B or 405B parameters using the following model names.\n","\n","* `meta-llama/Meta-Llama-3.1-70B-Instruct`\n","* `meta-llama/Meta-Llama-3.1-405B-Instruct`\n","\n","The 405B model is currently the best open model available. It is on par with GPT-4o and Claude 3. However, it will be much slower to run analyses with it. So, instead of looping through the entire dataset, consider running the same **one** prompt with this and other models."]},{"cell_type":"code","execution_count":null,"id":"23fab45f-b94a-434e-9de0-527c1287613a","metadata":{"id":"23fab45f-b94a-434e-9de0-527c1287613a"},"outputs":[],"source":["# start with the 70B version\n","LLM_version = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"\n","LLM = InferenceClient(model = LLM_version, token = API_TOKEN)"]},{"cell_type":"markdown","id":"HElsynQ56i4x","metadata":{"id":"HElsynQ56i4x"},"source":["## Select a decision reason for comparioson of models"]},{"cell_type":"code","execution_count":null,"id":"r6l6bz199fwe","metadata":{"id":"r6l6bz199fwe"},"outputs":[],"source":["# Set up the prompts for a decision reason of your choice\n","# here are the avilable reasons\n","disp_tab(decision_reasons)\n","\n","# change to your reason of choice\n","selected_reason = 'loss aversion'\n","\n","# reason description\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]"]},{"cell_type":"markdown","id":"rxLOVwJcARFB","metadata":{"id":"rxLOVwJcARFB"},"source":["### Run the reason through the model"]},{"cell_type":"code","execution_count":null,"id":"Qkc890nt9Q_a","metadata":{"id":"Qkc890nt9Q_a"},"outputs":[],"source":["# Create a list for storing prompts for the expected value reason\n","filled_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base,\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    filled_prompts.append(prompt)\n","\n","# list for storing the output from the model 1\n","LLM_results = []\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","problems_reports[selected_reason] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts):\n","\n","    # response from LLAMA\n","    LLM_response = LLM.text_generation(prompt, max_new_tokens = 4000)\n","    LLM_results.append(LLM_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, selected_reason] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"]},{"cell_type":"markdown","id":"fwxsBs4IE03h","metadata":{"id":"fwxsBs4IE03h"},"source":["### Evaluate the results"]},{"cell_type":"code","execution_count":null,"id":"hj4EbWNAE_mk","metadata":{"id":"hj4EbWNAE_mk"},"outputs":[],"source":["# Show verbal reports with low and high confidence\n","show_verbal_reports_in_range(problems_reports, selected_reason, 20, 80) # remove 20, 80 to show all output"]},{"cell_type":"code","execution_count":null,"id":"i5yXlh4wF-pR","metadata":{"id":"i5yXlh4wF-pR"},"outputs":[],"source":["print(LLM_results[15]) # response to verbal report 1 from model 1"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":5}