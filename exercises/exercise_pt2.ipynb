{"cells":[{"cell_type":"markdown","id":"efc8d5804ede6511","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"efc8d5804ede6511"},"source":["# Part 2: Choose your adventure\n","In this part you can explore differnt ways of improving the overall performance of the identification of decision reasons:\n","\n","- a) better model --- in this section you can test models other than LLAMA and compare the performance between them\n","- b) improve prompt --- in this section you can edit our prompt, or even writ your own from scratch\n","- c) other reasons --- in this section you can test your own decision reasons\n","\n"]},{"cell_type":"markdown","id":"c616368742ccde73","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"c616368742ccde73"},"source":["# Environment Setup"]},{"cell_type":"markdown","source":["## Monut google drive"],"metadata":{"id":"Ncfzwq_D1ElT"},"id":"Ncfzwq_D1ElT"},{"cell_type":"code","execution_count":null,"id":"initial_id","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"initial_id","executionInfo":{"status":"error","timestamp":1722067056252,"user_tz":-120,"elapsed":11605,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}},"outputId":"d3cf02d4-6293-4768-e251-d1fbefe30206"},"outputs":[{"output_type":"error","ename":"MessageError","evalue":"Error: credential propagation was unsuccessful","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3d47a21b8708>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# mount googl drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"]}],"source":["# mount googl drive\n","from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","source":["## Import the packacges and set the working directory"],"metadata":{"id":"1k98J5oh1HRh"},"id":"1k98J5oh1HRh"},{"cell_type":"code","source":["import sys\n","import os\n","import re\n","import textwrap\n","from IPython.display import display, HTML\n","import pandas as pd\n","from huggingface_hub import InferenceClient\n","\n","# the code below installs huggingface hub if it's missing\n","if 'google.colab' in sys.modules:  # If in Google Colab environment\n","\n","    # Installing requisite packages\n","    !pip install huggingface_hub &> /dev/null\n","\n","# this sets the working directory to the exercises folder\n","base_path = '/content/drive/My Drive/llms_egproc/exercises/'\n","os.chdir(base_path)"],"metadata":{"id":"pEeXB1mE0zaF"},"id":"pEeXB1mE0zaF","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import the functions we used in excercise 1 from the llm_funs.py file"],"metadata":{"id":"vooNk6-M1MT_"},"id":"vooNk6-M1MT_"},{"cell_type":"code","source":["# Add the script directory to the system path\n","sys.path.append(base_path)\n","\n","# custom funcions from ex1\n","from llm_funs import generate_prompt, extract_confidence, wrap_text, show_verbal_reports_in_range"],"metadata":{"id":"SN9UW2II0p7h"},"id":"SN9UW2II0p7h","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Set the access token"],"metadata":{"id":"Rd8S1JlvBSSt"},"id":"Rd8S1JlvBSSt"},{"cell_type":"code","source":["API_TOKEN = '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'"],"metadata":{"id":"jLvKZW5BBVbw"},"id":"jLvKZW5BBVbw","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Read in data"],"metadata":{"id":"2ZeNJuWk6KKr"},"id":"2ZeNJuWk6KKr"},{"cell_type":"code","source":["# read in decision problems, decision reasons, and verbal reports\n","decision_problems = pd.read_csv('data/decision_problems.csv', encoding = 'utf-8')\n","decision_reasons = pd.read_csv('data/decision_reasons.csv', encoding = 'utf-8')\n","verbal_reports = pd.read_csv('data/verbal_reports.csv', encoding = 'utf-8')\n","\n","# merge verbal reports with decision problems\n","problems_reports = pd.merge(decision_problems, verbal_reports, on = 'problem_id')\n","\n","# the prompt\n","prompt_path = 'prompts/prompt_v1.txt'\n","\n","# Open the file and read its contents\n","with open(prompt_path, 'r') as file:\n","    prompt_base= file.read()"],"metadata":{"id":"KqENHCDv6MQm"},"id":"KqENHCDv6MQm","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the functions from ex1"],"metadata":{"id":"jjoaqwCSDfBa"},"id":"jjoaqwCSDfBa"},{"cell_type":"code","source":["# function for constructing the full prompt\n","def generate_prompt(prompt, decision_problem, decision_reason, verbal_report):\n","    \"\"\"\n","    Replaces placeholders in the prompt with the given decision problem, decision reason, and verbal report.\n","    \"\"\"\n","    # Replace placeholders with actual values\n","    filled_prompt = prompt.replace(\"DECISION_PROBLEM\", decision_problem)\n","    filled_prompt = filled_prompt.replace(\"DECISION_REASON\", decision_reason)\n","    filled_prompt = filled_prompt.replace(\"VERBAL_REPORT\", verbal_report)\n","\n","    return filled_prompt\n","\n","# Function for extracting confidence assessments\n","def extract_confidence(s):\n","    \"\"\"\n","    Extracts an integer value from a string enclosed between @ or @@ symbols.\n","    \"\"\"\n","    # Regular expression to match patterns like @number@ or @@number@@\n","    pattern = r'@+(\\s*\\d+\\s*)@+'\n","\n","    # Search for the pattern in the string\n","    match = re.search(pattern, s)\n","\n","    if match:\n","        # Extract the number and convert it to an integer\n","        number_str = match.group(1).strip()\n","        return int(number_str)\n","\n","    return None\n","\n","# Function to wrap text\n","def wrap_text(text, width=100):\n","    return \"<br>\".join(textwrap.wrap(text, width))\n","\n","# display data frames in HTML\n","def disp_tab(dd):\n","    dd = dd.to_html(escape=False)\n","    return display(HTML(dd))\n","\n","# Function to show verbal reports with assigned numbers in a specified range\n","def show_verbal_reports_in_range(data, reason, min_confidence, max_confidence):\n","    \"\"\"\n","    Shows verbal reports for which the model assigned a confidence within the specified range.\n","    \"\"\"\n","    filtered_data = data[(data[reason] >= min_confidence) & (data[reason] <= max_confidence)] # filter by the specified range\n","\n","     # wrap the text for nicer display\n","    filtered_data.loc[:, 'verbal_report'] = filtered_data['verbal_report'].apply(wrap_text)\n","    filtered_data.loc[:, 'decision_problem'] = filtered_data['decision_problem'].apply(lambda x: wrap_text(x, width=40))\n","\n","    # select only the columns with report and confidence assesment\n","    filtered_data = filtered_data[['decision_problem', 'verbal_report', 'choice', reason]]\n","\n","    return disp_tab(filtered_data)\n","    # return filtered_data[['verbal_report', reason]]"],"metadata":{"id":"JbJQyYkpDvPd"},"id":"JbJQyYkpDvPd","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"4830e24a5d45430d","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"4830e24a5d45430d"},"source":["# Adeventure A: Better model"]},{"cell_type":"markdown","source":["## Select a decision reason for comparioson of models"],"metadata":{"id":"HElsynQ56i4x"},"id":"HElsynQ56i4x"},{"cell_type":"code","source":["# Set up the prompts for a decision reason of your choice\n","# here are the avilable reasons\n","disp_tab(decision_reasons)"],"metadata":{"id":"r6l6bz199fwe"},"id":"r6l6bz199fwe","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Generate prompts with the selected reason"],"metadata":{"id":"rxLOVwJcARFB"},"id":"rxLOVwJcARFB"},{"cell_type":"code","source":["selected_reason = 'maximum outcome' # change to your reason of choice\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]\n","\n","# Create a list for storing prompts for the expected value reason\n","filled_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base,\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    filled_prompts.append(prompt)"],"metadata":{"id":"Qkc890nt9Q_a"},"id":"Qkc890nt9Q_a","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(filled_prompts[0])"],"metadata":{"id":"W29O0-SZCyzJ"},"id":"W29O0-SZCyzJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run the analysis with model from excercise 1"],"metadata":{"id":"ekPBcBOMAgt1"},"id":"ekPBcBOMAgt1"},{"cell_type":"code","source":["LLM1_version = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n","LLM1 = InferenceClient(model = LLM1_version, token = API_TOKEN)\n","\n","# list for storing the output from the model 1\n","LLM1_results = []\n","\n","# column name for storage of the confidence values\n","llm1_res_col = 'llm1_confidence_res'\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","problems_reports[llm1_res_col] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts):\n","\n","    # response from LLAMA\n","    LLM1_response = LLM1.text_generation(prompt, max_new_tokens = 4000)\n","    LLM1_results.append(LLM1_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM1_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, llm1_res_col] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"id":"PO-UB_jY6DNc"},"id":"PO-UB_jY6DNc","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run the analysis with another model\n","We propose to use the recently upgraded version of the model used in ex1 and above"],"metadata":{"id":"oyRSTcVoB8Js"},"id":"oyRSTcVoB8Js"},{"cell_type":"code","source":["LLM2_version = \"meta-llama/Meta-Llama-3.1-70B-Instruct\"  # most recent updated LLAMA family\n","LLM2 = InferenceClient(model = LLM2_version, token = API_TOKEN)\n","\n","# list for storing the output from the model 1\n","LLM2_results = []\n","\n","# column name for storage of the confidence values\n","llm2_res_col = 'llm2_confidence_res'\n","\n","# new column in the problems_reports data set for storing the confidence assesments\n","problems_reports[llm2_res_col] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts):\n","\n","    # response from LLAMA\n","    LLM2_response = LLM2.text_generation(prompt, max_new_tokens = 4000)\n","    LLM2_results.append(LLM2_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM2_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, llm2_res_col] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"id":"tVekGHAqCC9V"},"id":"tVekGHAqCC9V","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Compare the results from both models\n","First, high confidence reports from model 1"],"metadata":{"id":"fwxsBs4IE03h"},"id":"fwxsBs4IE03h"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'llm1_confidence_res', 80, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"hj4EbWNAE_mk"},"id":"hj4EbWNAE_mk","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Second, high confidence reports from model 2"],"metadata":{"id":"2-WGxYppFWXH"},"id":"2-WGxYppFWXH"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'llm2_confidence_res', 50, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"h-C7MpaRFafu"},"id":"h-C7MpaRFafu","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can also compare the full text of the model 'thinking' directly, for corresponding verbal reports:\n","\n","(change the index numer in both code snippets to compare resposned corresponding to the same verbal reports)"],"metadata":{"id":"yqDWqG_IF5Do"},"id":"yqDWqG_IF5Do"},{"cell_type":"code","source":["print(LLM1_results[1]) # response to verbal report 1 from model 1"],"metadata":{"id":"i5yXlh4wF-pR"},"id":"i5yXlh4wF-pR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(LLM2_results[1]) # response to verbal report 1 from model 2"],"metadata":{"id":"rOYZyXa1GOND"},"id":"rOYZyXa1GOND","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Other ways of comparing the models\n","Think of other ways of comparing the model performance and explore them. Feel free to ask us questions on how to set up what you have in mind."],"metadata":{"id":"B6tMod4fFpPj"},"id":"B6tMod4fFpPj"},{"cell_type":"markdown","id":"81690eb2b6590715","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"81690eb2b6590715"},"source":["# Adeventure B: improve prompt\n","\n","- or make it worse (e.g., remove chain of thoughts)\n","- see how the model reacts to changes in the prompts\n"]},{"cell_type":"code","source":["# We should use the same model to compare the diiferences a prompt can make:\n","LLM_version = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n","LLM = InferenceClient(model = LLM_version, token = API_TOKEN)"],"metadata":{"id":"hC8lGoIDIbrQ"},"id":"hC8lGoIDIbrQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Also let's fix the decision reason"],"metadata":{"id":"6jXK9JXPJBVf"},"id":"6jXK9JXPJBVf"},{"cell_type":"code","source":["# Set up the prompts for a decision reason of your choice\n","# here are the avilable reasons\n","disp_tab(decision_reasons)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"id":"2fE6-N5bJH9h","executionInfo":{"status":"ok","timestamp":1722033294825,"user_tz":-120,"elapsed":10,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}},"outputId":"37161559-1447-4cc1-fb1e-8a927140fe1f"},"id":"2fE6-N5bJH9h","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>decision reason name</th>\n","      <th>decision reason description</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>minimum outcome</td>\n","      <td>The reason considers the minimum outcome of each lottery. The reason prefers the lottery with the more favorable minimum outcome.</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>maximum outcome</td>\n","      <td>The reason considers the maximum outcome of each lottery. The reason prefers the lottery with the more favorable maximum outcome.</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>most likely outcome</td>\n","      <td>The reason considers the most probable outcomes, that is the outcomes with the highest probability of each lottery. If there is more than one outcome with the highest probability, the more favorable one is considered. The reason prefers the lottery with the more favorable most probable outcome.</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>least likely outcome</td>\n","      <td>The reason considers the least probable outcomes, that is the outcomes with the lowest probability of each lottery. If there is more than one outcome with the lowest probability, the more favorable one is considered. The reason prefers the lottery with the more favorable least probable outcome.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>zero outcome probability</td>\n","      <td>The reason considers the probability of a zero outcome of each lottery. The reason prefers the lottery with the probability of the zero outcome that is more favorable in the context of the possible outcomes.</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>sure outcome</td>\n","      <td>The reason considers the presence of a sure outcome, that is an outcome with 100% probability, of each lottery. The reason prefers the lottery with or without the sure outcome, depending on whether the sure outcome is a favorable outcome in the context of all possible outcomes.</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>expected value</td>\n","      <td>The reason considers the expected value of each lottery. The reason prefers the lottery with the higher expected value.</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>loss aversion</td>\n","      <td>The reason considers the outcomes of each lottery. Losses are considered more important than gains. The reason prefers the lottery with the more favorable loss or, if losses are identical, the lower loss probability.</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>regret</td>\n","      <td>The reason considers the outcomes of each lottery. The sum of all pairwise differences of outcomes between the lotteries is considered important. The reason prefers the lottery with the more favorable sum of outcome differences.</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}]},{"cell_type":"code","source":["# set the decision reason\n","selected_reason = 'maximum outcome' # change to your reason of choice\n","selected_description = decision_reasons.loc[decision_reasons['decision reason name'] == selected_reason, 'decision reason description'].values[0]"],"metadata":{"id":"3nIuj1TcJKSs"},"id":"3nIuj1TcJKSs","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## First get model responses for our prompt\n"],"metadata":{"id":"Kse5y9JdG9_p"},"id":"Kse5y9JdG9_p"},{"cell_type":"code","source":["# Create a list for storing prompts\n","filled_prompts_p1 = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base,\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    filled_prompts_p1.append(prompt)"],"metadata":{"id":"l20flR4jI1KL"},"id":"l20flR4jI1KL","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Second, run the model with our prompt\n","While you wait for the results, you can star working on your own prompt in the next code snippet."],"metadata":{"id":"w3lZBDRTJh8j"},"id":"w3lZBDRTJh8j"},{"cell_type":"code","source":["# list for storing the output for prompt 1\n","LLM_P1_results = []\n","\n","# column name for storage of the confidence values from prompt 1\n","llm_P1_res_col = 'llm_P1_confidence_res'\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","problems_reports[llm_P1_res_col] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts_p1):\n","\n","    # response from LLAMA\n","    LLM_P1_response = LLM1.text_generation(prompt, max_new_tokens = 4000)\n","    LLM_P1_results.append(LLM_P1_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM1_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, llm_P1_res_col] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hGf4HjluIkpN","executionInfo":{"status":"ok","timestamp":1722033315201,"user_tz":-120,"elapsed":732,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}},"outputId":"bbb6ad91-167f-4d0e-ebd7-90249848c375"},"id":"hGf4HjluIkpN","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0/15\n","1/15\n"]}]},{"cell_type":"markdown","source":["## Prepare your own prompt\n","Keep in mind that the function `generate_prompt` requiers that the `base prompt` contains the place holders for the decision reason, decision problem and verbal report in the following forms: DECISION_PROBLEM, DECISION_REASON, VERBAL_REPORT.\n","\n","The rest can be changed as you please."],"metadata":{"id":"oWOeCXdYJ6bh"},"id":"oWOeCXdYJ6bh"},{"cell_type":"code","source":["prompt_base_p2 = \"\"\"\n","<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n","\n","You are a decision analyst who accurately identifies whether decision reasons are present or absent in verbal reports of people written after they made a choice between two monetary lotteries of a decision problem.\n","\n","Available information —\n","A decision problem poses a choice between two lotteries, A and B, offering different monetary outcomes with different probabilities.\n","\n","A decision reason specifies a rule to decide which of the two lotteries is preferred by the reason. The decision reason prefers A or B or is indifferent between the lotteries.\n","\n","A verbal report written by an individual describes, in retrospect, the individual’s deliberation process used to choose one of the lotteries of the decision problem.\n","\n","\n","Task description —\n","Your task is to assess, based on the verbal report, whether the individual used the reason to make the decision.\n","The wording in the verbal report does not need to match the decision reason verbatim;\n","consider other wordings but make sure that the essence of the reason is clearly reflected by the verbal report.\n","Perform your analysis in three steps.\n","\n","Step 1: Asses if the decision reason can be applied to the decision problem.\n","Evaluate whether the information relevant to the decision reason can be derived from the lotteries' outcomes and probabilities and summarize this information.\n","Proceed to Step 2.\n","\n","Step 2: Assess the verbal report.\n","First, evaluate and summarize the outcome and probability information considered by the individual.\n","Second, evaluate and summarize the individual’s justification for the choice.\n","Focus on the described deliberation process and ignore information about the individual’s final choice.\n","Proceed to Step 3.\n","\n","Step 3: Assess confidence in the decision reason’s use.\n","First, compare the outcome and probability information relevant to the decision reason and those considered by the individual.\n","Second, compare the decision reason’s rule to the individual’s justification for the choice.\n","Based on these two comparisons, return a value between 0 (certainly not used) and 100 (certainly used), reflecting your confidence that the individual used the decision reason to make the decision.\n","\n","\n","Output format —\n","Return the results of your assessment in the following format.\n","Return the confidence value by inserting it between two @@ symbols.\n","Only insert numbers between 0 (certainly not used) and 100 (certainly used).\n","\n","Here is a template for the output format:\n","Confidence: @ insert confidence value @\n","\n","<|eot_id|><|start_header_id|>user<|end_header_id|>\n","\n","Consider the following decision problem, decision reason, and verbal report:\n","\n","\n","Decision problem ---\n","DECISION_PROBLEM\n","\n","\n","Decision reason ---\n","DECISION_REASON\n","\n","\n","Verbal report ---\n","VERBAL_REPORT\n","\n","\n","Task ---\n","Perform the confidence assessment step-by-step. Closely follow the steps previously outlined. Describe your reasoning before you arrive at an answer.\n","In the end, provide your confidence assessment that the decision reason was used by the individual using the specified output format.\n","<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n","\"\"\""],"metadata":{"id":"IO3DYbDVKOKR"},"id":"IO3DYbDVKOKR","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate new prompts based on your prompt P2 for the previously set decision reason"],"metadata":{"id":"IPmxnTaxLG9t"},"id":"IPmxnTaxLG9t"},{"cell_type":"code","source":["# Create a list for storing prompts\n","filled_prompts_p2 = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base_p2, # here we are now passing your prompt\n","        row['decision_problem'],\n","        selected_description,  # Use the selected description\n","        row['verbal_report']\n","    )\n","    filled_prompts_p2.append(prompt)"],"metadata":{"id":"UeOc81o6LPHC"},"id":"UeOc81o6LPHC","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run the model"],"metadata":{"id":"EZuttxyKLXap"},"id":"EZuttxyKLXap"},{"cell_type":"code","source":["# list for storing the output for prompt 2\n","LLM_P2_results = []\n","\n","# column name for storage of the confidence values from prompt 1\n","llm_P2_res_col = 'llm_P2_confidence_res'\n","\n","# new column in the problems_reports data set for stroting the confidence assesments\n","problems_reports[llm_P2_res_col] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts_p2):\n","\n","    # response from LLAMA\n","    LLM_P2_response = LLM1.text_generation(prompt, max_new_tokens = 4000)\n","    LLM_P2_results.append(LLM_P2_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM_P2_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, llm_P2_res_col] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5mCv5tgHLaSH","executionInfo":{"status":"ok","timestamp":1722033438534,"user_tz":-120,"elapsed":27422,"user":{"displayName":"Kamil Fuławka","userId":"16000584420062196934"}},"outputId":"d48b790b-7c50-40b9-df9f-6262c08c5cc2"},"id":"5mCv5tgHLaSH","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0/15\n","1/15\n"]}]},{"cell_type":"markdown","source":["## Compare results\n","First have a look at results generated with our prompt"],"metadata":{"id":"iT0QvJpuMZuN"},"id":"iT0QvJpuMZuN"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'llm_P1_confidence_res', 80, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"wAz52SGLMdal"},"id":"wAz52SGLMdal","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now have a look at the results generated with your prompt"],"metadata":{"id":"IAEqSu6SMpEp"},"id":"IAEqSu6SMpEp"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, 'llm_P2_confidence_res', 80, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"Xyrqz_WcMs2m"},"id":"Xyrqz_WcMs2m","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can also compare the full text of the model 'thinking' directly, for corresponding verbal reports:\n","\n","(change the index numer in both code snippets to compare resposned corresponding to the same verbal reports)"],"metadata":{"id":"kk-9QX2yM1Zq"},"id":"kk-9QX2yM1Zq"},{"cell_type":"code","source":["print(LLM_P1_results[1]) # first LLM response based on prompt P1 (ours)"],"metadata":{"id":"cpfVg47jM2ho"},"id":"cpfVg47jM2ho","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(LLM_P2_results[1]) # first LLM response based on prompt P1 (ours)"],"metadata":{"id":"tU9N9IfsNJjf"},"id":"tU9N9IfsNJjf","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"1d00f880-e9b0-44f5-9328-a4fc1e39a815","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false},"id":"1d00f880-e9b0-44f5-9328-a4fc1e39a815"},"source":["# Adventure C: other reasons\n","\n","In this adventure you can test your own reasons."]},{"cell_type":"code","source":["# Let's set the InferenceClient first.\n","LLM_version = \"meta-llama/Meta-Llama-3-70B-Instruct\"\n","LLM = InferenceClient(model = LLM_version, token = API_TOKEN)"],"metadata":{"id":"ncKYB7rHN5MH"},"id":"ncKYB7rHN5MH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Have a look at our reasons for inspiration"],"metadata":{"id":"uAVGkuejN7k9"},"id":"uAVGkuejN7k9"},{"cell_type":"code","source":["# Set up the prompts for a decision reason of your choice\n","# here are the avilable reasons\n","disp_tab(decision_reasons)"],"metadata":{"id":"RpF0_pKGN_wE"},"id":"RpF0_pKGN_wE","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now create your own reason by filling in the code in the next snippet"],"metadata":{"id":"kSRiMItUOTIF"},"id":"kSRiMItUOTIF"},{"cell_type":"code","source":["# set the decision reason\n","new_reason = 'regret' # change to the name of your reason --- this will be used as a column name in the problems_reasons data for storing the confidence assesments\n","\n","# now add a description --- this description will be used in the prompt!\n","new_reason_description = 'The reason considers the outcomes of each lottery. The sum of all pairwise differences of outcomes between the lotteries is considered important. The reason prefers the lottery with the more favorable sum of outcome differences.'"],"metadata":{"id":"wlJ-skarOYyM"},"id":"wlJ-skarOYyM","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generate the prompts with your reason"],"metadata":{"id":"U4vqFzU_OyLk"},"id":"U4vqFzU_OyLk"},{"cell_type":"code","source":["# Create a list for storing prompts\n","filled_prompts = []\n","\n","# Generate prompts for the specific decision reason\n","for _, row in problems_reports.iterrows():\n","\n","    # here we are using the generate prompt function to create prompts for all verbal reports and the expected value reason\n","    prompt = generate_prompt(\n","        prompt_base, # here we are now passing your prompt\n","        row['decision_problem'],\n","        new_reason_description,  # Use the selected description that you provided above\n","        row['verbal_report']\n","    )\n","    filled_prompts.append(prompt)"],"metadata":{"id":"QqRpnJ9xO3nd"},"id":"QqRpnJ9xO3nd","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(filled_prompts[1])"],"metadata":{"id":"bZhV0I8cPKC5"},"id":"bZhV0I8cPKC5","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run the model"],"metadata":{"id":"iZ1B_pk8Pl4U"},"id":"iZ1B_pk8Pl4U"},{"cell_type":"code","source":["# list for storing the output\n","LLM_results = []\n","\n","# column name for storage of the confidence values will be the name under `selected_reason`\n","problems_reports[new_reason] = None\n","\n","# run the analysis\n","# Iterate over the list of prompts, get responses, and extract numerical estimates and add them to the data set with problems and reports\n","for i, prompt in enumerate(filled_prompts):\n","\n","    # response from LLAMA\n","    LLM_response = LLM.text_generation(prompt, max_new_tokens = 4000)\n","    LLM_results.append(LLM_response) # save the response to the sure_outcome_eval list\n","\n","    # extract the confidence value from the response\n","    confidence_assesment = extract_confidence(LLM_response)\n","\n","    # confidence value into the data\n","    problems_reports.at[i, new_reason] = confidence_assesment\n","\n","    # monitor progress\n","    print(str(i) + '/' + str(problems_reports.shape[0]-1))"],"metadata":{"id":"_oUyoKlbPq6f"},"id":"_oUyoKlbPq6f","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Check the results!"],"metadata":{"id":"f-atOaVQQY9x"},"id":"f-atOaVQQY9x"},{"cell_type":"code","source":["# Show verbal reports for which the reason was assessed to be used with high confidecne, i.e., between 80 to 100\n","show_verbal_reports_in_range(problems_reports, new_reason, 80, 100) # or use 0, 100 to display the entier data frame"],"metadata":{"id":"ANMGnmGZQbG6"},"id":"ANMGnmGZQbG6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(LLM_results[7]) # first LLM response based"],"metadata":{"id":"mGphSUGMQeb2"},"id":"mGphSUGMQeb2","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Clean the notebook (optional)"],"metadata":{"id":"17TEcSu44RLz"},"id":"17TEcSu44RLz"},{"cell_type":"code","source":["# Clear all variables\n","%reset -f\n","\n","# Clear all outputs\n","from IPython.display import clear_output\n","clear_output()\n","\n","# Restart runtime\n","import os\n","os._exit(00)"],"metadata":{"id":"n6c6cbb54T2i"},"id":"n6c6cbb54T2i","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[],"collapsed_sections":["c616368742ccde73","4830e24a5d45430d","81690eb2b6590715","1d00f880-e9b0-44f5-9328-a4fc1e39a815"]}},"nbformat":4,"nbformat_minor":5}